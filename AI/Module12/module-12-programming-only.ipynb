{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "There are general instructions on Blackboard and in the Syllabus for Programming Assignments. This Notebook also has instructions specific to this assignment. Read all the instructions carefully and make sure you understand them. Please ask questions on the discussion boards or email me at `EN605.445@gmail.com` if you do not understand something.\n",
    "\n",
    "<div style=\"background: mistyrose; color: firebrick; border: 2px solid darkred; padding: 5px; margin: 10px;\">\n",
    "You must follow the directions *exactly* or you will get a 0 on the assignment.\n",
    "</div>\n",
    "\n",
    "You must submit a zip file of your assignment and associated files (if there are any) to Blackboard. The zip file will be named after you JHED ID: `<jhed_id>.zip`. It will not include any other information. Inside this zip file should be the following directory structure:\n",
    "\n",
    "```\n",
    "<jhed_id>\n",
    "    |\n",
    "    +--module-01-programming.ipynb\n",
    "    +--module-01-programming.html\n",
    "    +--(any other files)\n",
    "```\n",
    "\n",
    "For example, do not name  your directory `programming_assignment_01` and do not name your directory `smith122_pr1` or any else. It must be only your JHED ID. Make sure you submit both an .ipynb and .html version of your *completed* notebook. You can generate the HTML version using:\n",
    "\n",
    "> ipython nbconvert [notebookname].ipynb\n",
    "\n",
    "or use the File menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "In this assignment you will be using the mushroom data from the Decision Tree module:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "The assignment is to write a program that will learn and apply a Naive Bayes Classifier for this problem. You'll first need to calculate all of the necessary probabilities (don't forget to use +1 smoothing) using a `learn` function. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple is a class and the *normalized* probability of that class. The List should be sorted so that the probabilities are in descending order. For example,\n",
    "\n",
    "```\n",
    "[(\"e\", 0.98), (\"p\", 0.02)]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class with the highest probability (the first one in the list).\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the un-normalized probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You'll also need an `evaluate` function as before. You should use the $error\\_rate$ again.\n",
    "\n",
    "Use the same testing procedure as last time, on two randomized subsets of the data:\n",
    "\n",
    "1. learn the probabilities for set 1\n",
    "2. classify set 2\n",
    "3. evaluate the predictions\n",
    "4. learn the probabilities for set 2\n",
    "5. classify set 1\n",
    "6. evalute the the predictions\n",
    "7. average the classification error.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Metadata###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the `agaricus-lepiota.names` file, the metadata is encoded as follows. This is necessary due to the possibility of not encountering every feature values in the training data and having to the cardinality of each features for probability smoothing. Both the sample space of the features as well as the column headers are encoded below.\n",
    "\n",
    "The possible values of features are encoded in a dictionary where the keys are the feature names and the values are set of possible values. The column names are encoded in a list with the same order as it is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureVals = {\n",
    "    \"class\": set(['p', 'e']),\n",
    "    \"cap-shape\": set([\"b\",\"c\",\"x\",\"f\",\"k\",\"s\"]),\n",
    "    \"cap-surface\": set([\"f\",\"g\",\"y\",\"s\"]),\n",
    "    \"cap-color\": set([\"n\",\"b\",\"c\",\"g\",\"r\",\"p\",\"u\",\"e\",\"w\",\"y\"]),\n",
    "    \"bruises\": set([\"t\",\"f\"]),\n",
    "    \"odor\": set([\"a\",\"l\",\"c\",\"y\",\"f\",\"m\",\"n\",\"p\",\"s\"]),\n",
    "    \"gill-attachment\": set([\"a\",\"d\",\"f\",\"n\"]),\n",
    "    \"gill-spacing\": set([\"c\",\"w\",\"d\"]),\n",
    "    \"gill-size\": set([\"b\",\"n\"]),\n",
    "    \"gill-color\": set([\"k\",\"n\",\"b\",\"h\",\"g\",\"r\",\"o\",\"p\",\"u\",\"e\",\"w\",\"y\"]),\n",
    "    \"stalk-shape\": set([\"e\",\"t\"]),\n",
    "    \"stalk-root\": set([\"b\",\"c\",\"u\",\"e\",\"z\",\"r\",\"?\"]),\n",
    "    \"stalk-surface-above-ring\": set([\"f\",\"y\",\"k\",\"s\"]),\n",
    "    \"stalk-surface-below-ring\": set([\"f\",\"y\",\"k\",\"s\"]),\n",
    "    \"stalk-color-above-ring\": set([\"n\",\"b\",\"c\",\"g\",\"o\",\"p\",\"e\",\"w\",\"y\"]),\n",
    "    \"stalk-color-below-ring\": set([\"n\",\"b\",\"c\",\"g\",\"o\",\"p\",\"e\",\"w\",\"y\"]),\n",
    "    \"veil-type\": set([\"p\",\"u\"]),\n",
    "    \"veil-color\": set([\"n\",\"o\",\"w\",\"y\"]),\n",
    "    \"ring-number\": set([\"n\",\"o\",\"t\"]),\n",
    "    \"ring-type\": set([\"c\",\"e\",\"f\",\"l\",\"n\",\"p\",\"s\",\"z\"]),\n",
    "    \"spore-print-color\": set([\"k\",\"n\",\"b\",\"h\",\"r\",\"o\",\"u\",\"w\",\"y\"]),\n",
    "    \"population\": set([\"a\",\"c\",\"n\",\"s\",\"v\",\"y\"]),\n",
    "    \"habitat\": set([\"g\",\"l\",\"m\",\"p\",\"u\",\"w\",\"d\"])\n",
    "}\n",
    "\n",
    "headers = ['class', 'cap-shape', 'cap-surface', 'cap-color' , 'bruises', 'odor', \n",
    "           'gill-attachment' , 'gill-spacing', 'gill-size', 'gill-color' , 'stalk-shape',\n",
    "           'stalk-root', 'stalk-surface-above-ring' , 'stalk-surface-below-ring',\n",
    "           'stalk-color-above-ring', 'stalk-color-below-ring' , 'veil-type', 'veil-color',\n",
    "           'ring-number' , 'ring-type', 'spore-print-color', 'population', 'habitat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data ###\n",
    "\n",
    "Read the CSV file in as numpy recarray where the record names are the headers as encoded above. The sequence of the data is then randomized and split into two subsets via slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readCSV(filePath):\n",
    "    with open(filePath, 'rt') as f:\n",
    "        reader = csv.reader(f)\n",
    "        l = list(reader)\n",
    "    return l\n",
    "\n",
    "raw = np.rec.fromrecords( readCSV('agaricus-lepiota.data') , names=headers)\n",
    "\n",
    "idx = np.array(range(raw.size))\n",
    "np.random.shuffle(idx) # shuffle the indices\n",
    "data1 = raw[ idx[:int(np.floor(raw.size/2))] ] # first half of data\n",
    "data2 = raw[ idx[int(np.floor(raw.size/2)):] ] # second half of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm ###\n",
    "\n",
    "**condProb(data, N, meta)**  \n",
    "The function takes input of a data in the form of a recarray, an integer denoting the number of records having the current class value, and the metadata which is a tuple containing the dictionary of feature values and list of headers. The recarray is assumed to have homogenous class values, as the function is used to calculate probabilities conditioned on a record having a specific class value. \n",
    "\n",
    "The function iterate through all possible features (using the header in the metadata parameter) and for each feature, calculates the conditional probability as follows:\n",
    "\n",
    "$$Pr(x_i|C=c)=\\frac{\\#(x_i,c)+1}{\\#c+|x|+1}$$\n",
    "\n",
    "where $|x|$ is the cardinality of the feature.\n",
    "\n",
    "The conditional probability of each feature would be stored in a dictionary whose keys are the feature values. The dictionary for each feature will be keyed by the feature name in a encompassing dictionary and returned.\n",
    "\n",
    "\n",
    "**learn(data)**  \n",
    "The `learn` function takes input of a data in the form of a recarray and the metadata which is a tuple containing the dictionary of feature values and list of headers. \n",
    "\n",
    "The function iterate through all possible class values, and for each value, it calculates the conditional probability of the data having a specific feature value given that it has the specified class value via the function `condProb`. Each iteration the function slices the input data to ensure homogeneity in the data.\n",
    "\n",
    "The output is a dictionary with two keys, `prob` and `cond`. The `prob` key has the values of another dict which has the keys of class values and dictionary value of unconditional probabilities for these class values. Under the `cond` key is another dictionary whose keys are the feature names with values of another dictionary containing the conditional probabilities of the various values conditioned on having a specific class value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def condProb(data, N, meta): # the data is assumed to be class-homogenous\n",
    "    features = set(meta[0]) - set(['class'])\n",
    "    featureVals = meta[1]\n",
    "    condPr = dict()\n",
    "    for f in features:\n",
    "        card = len(featureVals[f]) # cardinality of class\n",
    "        defProb = 1 / (N + card + 1)\n",
    "        tmp = dict([(v,defProb) for v in featureVals[f]]) # initialize base\n",
    "        vals, vN = np.unique(data[f], return_counts=True)\n",
    "        vPr = (vN + 1) / (N + card + 1)\n",
    "        for i,v in enumerate(vals):\n",
    "            tmp[v] = vPr[i]\n",
    "        condPr[f] = tmp\n",
    "\n",
    "    return condPr\n",
    "\n",
    "def learn(data, meta):\n",
    "    pr = dict()\n",
    "    classes,classN = np.unique(data['class'], return_counts=True )\n",
    "    tmp = (classN+1) / (data.size + len(classes)+ 1)\n",
    "    \n",
    "    pr['prob'] = dict( [(c,v) for c,v in zip(classes,tmp)] )\n",
    "    pr['cond'] = dict()\n",
    "    for x,c in enumerate(classes):\n",
    "        idx = data['class']==c;\n",
    "        pr['cond'][c] = condProb(data[idx], np.sum(idx), meta)\n",
    "        \n",
    "    return pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification algorithm ###\n",
    "\n",
    "The function takes three input: the conditional probabilities as calculated by `learn(.)` function, a data to be classified as a numpy recarray, and a metadata of all feature values encoded as a dict.\n",
    "\n",
    "The function calculates $Pr(c)=\\prod_{i,j}{p(a_i=v_j|c)}$ for each of the class values by iterating through each of the records and calculating the cumulative product of all the conditional probabilities and the unconditional probability of a class value.\n",
    "\n",
    "The posterior probabilities for each record are then normalized, sorted in descending order, and put into a list of list of tuples containing both the class value and the noramlized probability a la `[(\"e\", 0.98), (\"p\", 0.02)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(pr, data, featureVals):\n",
    "    features = [h for h in data.dtype.names if not h=='class']\n",
    "    classes = np.array(list(featureVals['class']))\n",
    "    \n",
    "    tmp = np.zeros([data.size, len(classes)])\n",
    "    for i,c in enumerate(classes):\n",
    "        conds = pr['cond'][c]\n",
    "        base = [pr['prob'][c]]\n",
    "        for n in range(data.size):\n",
    "            tmp[n][i] = np.cumprod(base + \n",
    "                                [conds[f][data[n][f]] for f in features])[-1]\n",
    "            \n",
    "    normed = (tmp.T / np.sum(tmp, axis=1)).T # normalized probability\n",
    "    idx = np.argsort(-normed, axis=1)# index per row, sort by desc. prob\n",
    "    sortedClass = classes[idx] # classes\n",
    "    sortedProb = normed[np.tile(np.arange(data.size), [2,1]).T, idx] # prob\n",
    "    \n",
    "    return [[(a,b) for a,b in zip(k,p)] for k,p in zip(sortedClass, sortedProb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation algorithm ###\n",
    "The function calculates the error rate by calculating the instances of prediction not matching the actual class. Formally, it returns: \n",
    "$$\\frac{1}{n}\\sum_{i}^{n}{I_{k_i\\ne\\hat{k_i}}(i\\in x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(data, classes):\n",
    "    act = data['class']\n",
    "    pred = np.array(classes)[:,0,0]\n",
    "    return np.sum(act!=pred) / data.size * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Classification and Evaluation ###\n",
    "\n",
    "Learning probability from set 1 and classifying set 2, and vise versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta = (headers, featureVals)\n",
    "prob1 = learn(data1, meta)\n",
    "pred2 = classify(prob1, data2, featureVals)\n",
    "prob2 = learn(data2, meta)\n",
    "pred1 = classify(prob2, data1, featureVals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the overall classification error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error rate: 5.11%\n"
     ]
    }
   ],
   "source": [
    "err = evaluate( np.hstack([data1,data2]), np.vstack([pred1,pred2]) )\n",
    "print('Average error rate: %.2f%%' % err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
