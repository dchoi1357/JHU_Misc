{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Information Retrieval\n",
    "\n",
    "Student: John Wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, re, nltk, time, math\n",
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Build in-memory inverted file\n",
    "\n",
    "The input document will be processed one by one, with the result being appended into a inverted file, which is a dictionary. These will be performed by 2 utilty functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDoc(txt, docID, vocab):\n",
    "    d = Counter( nltk.word_tokenize(txt) ) # count of each token (as by NLTK)\n",
    "    for tk in d: # merge dict of this doc with the bigger vocab dict\n",
    "        if tk not in vocab: # if not in vocab\n",
    "            vocab[tk] = [(docID, d[tk])] # first posting for token: (docID, DF)\n",
    "        else: # if already in vocab\n",
    "            vocab[tk].append( (docID, d[tk]) ) # append to posting list\n",
    "    return vocab, d\n",
    "\n",
    "def processDocsFile(docFile):\n",
    "    nDocs = 0 # count number of total docs processed\n",
    "    vcb = dict() # dict for inverted file\n",
    "\n",
    "    with open(docFile, 'r') as f:\n",
    "        for line in f: # NOTE: read line by line due to possibly large size\n",
    "            docID,txt = line.split('\\t')\n",
    "            docID = int(docID) # parse into int\n",
    "            vcb, tmpDict = processDoc(txt, docID, vcb) # process single doc\n",
    "            nDocs += 1\n",
    "\n",
    "        for term in vcb:  # go through dict and sort the posting lists\n",
    "            vcb[term].sort(key=itemgetter(0)) # sort by first elem, or docID\n",
    "            \n",
    "    return vcb, nDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the parsing of TIME dataset and building of inverted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "fName = './data/time-documents.txt'\n",
    "t0 = time.perf_counter()\n",
    "timeInv, timeNdocs = processDocsFile(fName)\n",
    "tt = time.perf_counter() - t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Posting List Tuples for Terms__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTER -> [(308, 1)]\n",
      "THAILAND -> [(203, 1), (243, 5), (280, 14), (396, 1), (449, 1), (498, 1), (516, 1), (534, 5), (543, 12), (544, 2)]\n",
      "ROCKETS -> [(27, 1), (117, 1), (186, 1), (313, 6), (404, 1), (464, 2), (495, 1), (509, 2), (545, 2)]\n"
     ]
    }
   ],
   "source": [
    "terms = ['COMPUTER', 'THAILAND', 'ROCKETS']\n",
    "for t in terms:\n",
    "    posts = timeInv[t][:10]\n",
    "    print('%s -> %s'%(t,posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Print DF and IDF__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPUTER: DF=1, IDF=1.000000\n",
      "THAILAND: DF=11, IDF=0.090909\n",
      "ROCKETS: DF=9, IDF=0.111111\n"
     ]
    }
   ],
   "source": [
    "for t in terms:\n",
    "    df = len(timeInv[t])\n",
    "    print('%s: DF=%d, IDF=%f'%(t,df,1/df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Timing of Processing Documents__\n",
    "\n",
    "The time is measured as CPU process time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 0 minutes and 6.278 seconds.\n"
     ]
    }
   ],
   "source": [
    "print('Processed in %d minutes and %.3f seconds.'%(tt//60,tt%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Document vector length\n",
    "\n",
    "The function below implement the algorithm provided in the assignment. Note that +1.0 is added to raw IDF so to not end up with 0 if a term appears in all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDocLens(vcb, nDocs):\n",
    "    docLens = Counter() # use dict since docID may not be contiguous\n",
    "    idfs = dict()\n",
    "    \n",
    "    for term,posts in vcb.items(): # loop over all terms in collection\n",
    "        idf = math.log2(1.0 + nDocs/len(posts)) # +1.0 for term in all docs\n",
    "        idfs[term] = (len(posts), idf)\n",
    "        for docID,tf in posts: # loop over docID and tf(term,docid)\n",
    "            docLens[docID] += (tf*idf)**2 # accumulate doc vector length\n",
    "            \n",
    "    for docID,accum in docLens.items(): # loop calculate proper doc vec length\n",
    "        docLens[docID] = math.sqrt(accum) # sqrt of sum of squared terms\n",
    "    \n",
    "    return docLens,idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Document Vector Lengths__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocID=17, length=187.101712\n",
      "DocID=18, length=71.207738\n",
      "DocID=19, length=155.398830\n",
      "DocID=20, length=75.471414\n",
      "DocID=21, length=185.960048\n",
      "DocID=23, length=145.982374\n",
      "DocID=24, length=243.981400\n",
      "DocID=25, length=73.511986\n",
      "DocID=26, length=135.834620\n",
      "DocID=27, length=84.270999\n"
     ]
    }
   ],
   "source": [
    "timeDocLens,timeIDFs = calcDocLens(timeInv, timeNdocs)\n",
    "tmp = sorted(timeDocLens.items(), key=itemgetter(0))[:10] # sorted by docID\n",
    "for docID,docLen in tmp: # print 10 lowest by numerical docID\n",
    "    print('DocID=%d, length=%f'%(docID,docLen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Query representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processQueryFile(queryFile):\n",
    "    with open(queryFile, 'r') as f:\n",
    "        txts = f.read().splitlines()\n",
    "    qs = [None for x in range(len(txts))]\n",
    "    qIDs = [0 for x in range(len(txts))]\n",
    "    for n,line in enumerate(txts):\n",
    "        qID,qTxt = line.split('\\t')\n",
    "        qIDs[n] = int(qID)\n",
    "        qs[n] = Counter(nltk.word_tokenize(qTxt))\n",
    "\n",
    "    return list(zip(qIDs, qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "fName = './data/time-queries.txt'\n",
    "timeQs = processQueryFile(fName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KENNEDY: tf=1, idf=3.321928\n",
      "ADMINISTRATION: tf=1, idf=4.539975\n",
      "PRESSURE: tf=1, idf=3.829723\n",
      "ON: tf=1, idf=1.071462\n",
      "NGO: tf=1, idf=4.469235\n",
      "DINH: tf=1, idf=4.469235\n",
      "DIEM: tf=1, idf=4.277338\n",
      "TO: tf=1, idf=1.001708\n",
      "STOP: tf=1, idf=3.872352\n",
      "SUPPRESSING: not found in Corpus\n",
      "THE: tf=1, idf=1.000000\n",
      "BUDDHISTS: tf=1, idf=5.179909\n",
      ".: tf=1, idf=1.000000\n",
      "\n",
      "Query Vector Length: 12.269275\n"
     ]
    }
   ],
   "source": [
    "qLen = 0\n",
    "for term,tf in timeQs[0][1].items():\n",
    "    if term in timeIDFs:\n",
    "        df,idf = timeIDFs[term]\n",
    "        print('%s: tf=%d, idf=%f'%(term,tf,idf))\n",
    "        qLen += (tf*idf) ** 2\n",
    "    else:\n",
    "        print('%s: not found in Corpus'%term)\n",
    "qLen = math.sqrt(qLen)\n",
    "    \n",
    "print('\\nQuery Vector Length: %f'%qLen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Score Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSim(qDict, invFile, idfs, docLens):\n",
    "    sims = Counter()  # counter for storing simularity scores\n",
    "    qLen = 0 # vector length of query\n",
    "    for tk,quTF in qDict.items(): # loop over terms in a query\n",
    "        if tk not in invFile: # skip query term if not in corpus\n",
    "            continue\n",
    "        df,idf = idfs[tk]\n",
    "        qLen += (quTF*idf) ** 2\n",
    "        for docID,corpTF in invFile[tk]: # iterate through posting list\n",
    "            sims[docID] += corpTF*idf * quTF*idf \n",
    "    \n",
    "    qLen = math.sqrt(qLen)\n",
    "    for docID in sims:\n",
    "        sims[docID] /= (docLens[docID] * qLen)\n",
    "    return sims # return simularity scores of each document (most are 0)\n",
    "\n",
    "def processQueries(qs, invFile, idfs, docLens):\n",
    "    scores = [None for x in range(len(qs))]\n",
    "    for n,(qID,qDict) in enumerate(qs):\n",
    "        scores[n] = (qID,cosineSim(qDict, invFile, idfs, docLens))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Processing Queries and Timing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 0 minutes and 0.506 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "timeQscores = processQueries(timeQs, timeInv, timeIDFs, timeDocLens)\n",
    "tt = time.perf_counter() - t0\n",
    "print('Processed in %d minutes and %.3f seconds.'%(tt/60,tt%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample of Cosine Similarity Scores__\n",
    "\n",
    "This shows the cosine similarity scores of the first query for 20 arbitrary document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryID: 17, similarity score = 0.078335\n",
      "QueryID: 21, similarity score = 0.061475\n",
      "QueryID: 28, similarity score = 0.065611\n",
      "QueryID: 29, similarity score = 0.071132\n",
      "QueryID: 43, similarity score = 0.067917\n",
      "QueryID: 45, similarity score = 0.067563\n",
      "QueryID: 57, similarity score = 0.047027\n",
      "QueryID: 62, similarity score = 0.088443\n",
      "QueryID: 67, similarity score = 0.052492\n",
      "QueryID: 70, similarity score = 0.055056\n",
      "QueryID: 71, similarity score = 0.087184\n",
      "QueryID: 105, similarity score = 0.057302\n",
      "QueryID: 126, similarity score = 0.068939\n",
      "QueryID: 163, similarity score = 0.083421\n",
      "QueryID: 183, similarity score = 0.120382\n",
      "QueryID: 188, similarity score = 0.075368\n",
      "QueryID: 196, similarity score = 0.095314\n",
      "QueryID: 204, similarity score = 0.068869\n",
      "QueryID: 217, similarity score = 0.050837\n",
      "QueryID: 221, similarity score = 0.075570\n"
     ]
    }
   ],
   "source": [
    "for n,(qID,s) in enumerate(timeQscores[0][1].items()):\n",
    "    if n>=20:\n",
    "        break\n",
    "    print('QueryID: %d, similarity score = %f'%(qID,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Ranked List\n",
    "Since we're using `Counter` to store similarity scores, we can use the built-in `most_common()` function, which implements a binary heap for extracting the top N items with highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopNSimDocs(qID, simScore, N=50): # return top N document for a query\n",
    "\ttopN = simScore.most_common(N) # use binary heap for extracting top N\n",
    "\tfmt = '%d Q0 %d %d %.6f jwu74\\n' # format for output file lines\n",
    "\treturn [fmt % (qID,docID,n+1,score) for n,(docID,score) in enumerate(topN)]\n",
    "\n",
    "def writeQueryResult(outName, qScores, N=50):\n",
    "    with open(outName, 'w') as fh:\n",
    "        for qInd,score in qScores: # loop over query results\n",
    "            out = getTopNSimDocs(qInd,score) # get top N docs based on sim\n",
    "            fh.writelines(out) # write out lines for output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputting query results to `time-jwu74.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeQueryResult('time-jwu74.txt', timeQscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryCorpus(corpusFile, queryFile, outFile):\n",
    "    t0 = time.perf_counter()\n",
    "    invFile, nDocs = processDocsFile(corpusFile)\n",
    "    docLens, idfs = calcDocLens(invFile, nDocs)\n",
    "    buildTime = time.perf_counter() - t0\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    qTxts = processQueryFile(queryFile)\n",
    "    qryScores = processQueries(qTxts, invFile, idfs, docLens)\n",
    "    queryTime = time.perf_counter() - t0\n",
    "    \n",
    "    writeQueryResult(outFile, qryScores)\n",
    "    \n",
    "    return buildTime, queryTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt, qt = queryCorpus('./data/fire10-documents.txt', \n",
    "                     './data/fire10-queries.txt', 'fire10-jwu74.txt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build time for fire10: 19 minutes 30.096 seconds\n",
      "Query time for fire10: 0 minutes 43.885 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Build time for fire10: %d minutes %.3f seconds'%(bt//60,bt%60))\n",
    "print('Query time for fire10: %d minutes %.3f seconds'%(qt//60,qt%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5//3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
