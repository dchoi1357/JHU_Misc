{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04\n",
    "\n",
    "student: John Wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, sys, csv, string, re, sklearn.preprocessing, sklearn.metrics\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn import naive_bayes as NB, svm as SVM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import IPython.display as disp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file names\n",
    "trainFile = './data/train.tsv'\n",
    "testFile = './data/test.tsv'\n",
    "devFile = './data/dev.tsv'\n",
    "varNames = ['stars','docID','text']\n",
    "\n",
    "# read in files\n",
    "train = pd.read_csv(trainFile, sep='\\t', header=None, names=varNames)\n",
    "dev = pd.read_csv(devFile, sep='\\t', header=None, names=varNames)\n",
    "test = pd.read_csv(testFile, sep='\\t', header=None, names=varNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Study the training data\n",
    "\n",
    "This section explores the training data set to allow a better understanding of the data. To start with, we look at whether the classes are balanced. If the mean rating of the data is 3, it would mean the data is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['stars'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some idea of useful features for the data, we use `CountVectorizer` to count the number of term frequencies of terms appearing in each document. We set the CountVectorizer to only return binary counts (i.e. value=1 if term is in the document at least once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binVec = CountVectorizer(tokenizer=nltk.word_tokenize, binary=True)\n",
    "binTF = binVec.fit_transform(train['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative word frequency\n",
    "In the following section, we find the top terms with the biggest difference of appearences between two and four-star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great: 39.60% (pos), 17.80% (neg)\n",
      "was: 54.90% (pos), 75.60% (neg)\n",
      "not: 42.40% (pos), 62.00% (neg)\n",
      "!: 47.90% (pos), 28.40% (neg)\n",
      "were: 27.60% (pos), 41.60% (neg)\n",
      "n't: 45.30% (pos), 59.10% (neg)\n",
      "always: 22.70% (pos), 9.80% (neg)\n",
      "good: 55.70% (pos), 42.90% (neg)\n",
      "did: 14.40% (pos), 27.10% (neg)\n",
      "be: 32.60% (pos), 44.20% (neg)\n",
      "just: 26.70% (pos), 38.10% (neg)\n",
      "better: 11.70% (pos), 22.80% (neg)\n",
      "delicious: 14.40% (pos), 3.30% (neg)\n",
      "friendly: 17.50% (pos), 6.60% (neg)\n",
      "are: 45.00% (pos), 34.20% (neg)\n",
      "because: 13.80% (pos), 24.10% (neg)\n",
      "ordered: 13.90% (pos), 24.10% (neg)\n",
      "no: 15.50% (pos), 25.50% (neg)\n",
      "bad: 7.20% (pos), 17.00% (neg)\n",
      "at: 38.60% (pos), 47.10% (neg)\n"
     ]
    }
   ],
   "source": [
    "twoSt = (train['stars']==2).to_numpy() # idx for 2-star reviews\n",
    "tfDiff = np.abs(binTF[twoSt].mean(axis=0) - binTF[~twoSt].mean(axis=0))\n",
    "top20idx = tfDiff.A1.argsort()[-20:][::-1] # get last 20, descending\n",
    "terms = binVec.get_feature_names() # get actual terms of doc matrix\n",
    "for x in top20idx: # loop over all terms in top 20 difference\n",
    "    a,b = binTF[~twoSt,x].mean()*100, binTF[twoSt,x].mean()*100\n",
    "    print('%s: %.2f%% (pos), %.2f%% (neg)'%(terms[x],a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms are listed in order of disparity. As expected, words like \"great\", \"always\", \"good\", \"delicious\", and \"friendly\" are expected to have a high presence in positive reviews, while terms like \"not\" and \"bad\" has a high presence in negative reviews. However, there are also some counter-intuitive examples. The term \"better\" is more frequently seen in negative reviews due to expressions like \"maybe the next time I come in the food will be better\". While the term \"like\" can connotate a favorable feeling, it is also used in simile, which are present in negative reviews such as \"it tastes like a combo of cream cheese, american cheese and sour cream\". \n",
    "\n",
    "Terms like \"was\", \"were\", and \"did\" also had a higher presence in negative reviews. These words are combined with others to form negative phrases like \"was not\" and \"weren't\". One unexpected result is that people are much more likely to use exclamation marks in postive reviews. The word \"ordered\" appear to be used more frequent in negative reviews. After looking through negative reviews, they often contain details which list the items ordered and how they are bad, such as \"my friend ordered a virgin strawberry daiquiri and instead she got some weird smoothie with whip cream on top\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other useful characteristics\n",
    "\n",
    "In this section, we explore a few characteristics that are different between the two types of reviews. Each cell we break down the characteristic by star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "2    720.375\n",
       "4    631.283\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textLen = train['text'].str.len() # length of text\n",
    "textLen.groupby(train['stars']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative reviews are 90 characters longer on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "2    0.025294\n",
       "4    0.027273\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capPct = train['text'].str.count(r'[A-Z]')/textLen # % of chars upper case\n",
    "capPct.groupby(train['stars']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive reviews tend to have a slightly larger proportion of upper case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "2    0.000543\n",
       "4    0.000399\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nPunct = train['text'].str.count('2') # number of apperences of number 2\n",
    "(nPunct/textLen).groupby(train['stars']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-star reviews tend to have more mentions of the number \"2\", likely from the reviews explicitly enumerating the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Train a classifier\n",
    "\n",
    "In this section, we build a pipeline for a naive Bayes classifier. The pipeline includes two pars:\n",
    "1. **TF-IDF vectorizer**, which extracts features from input text and builds a document-term matrix based on TF-IDF values. \n",
    "  * The text is tokenized via NLTK `word_tokenize` function.\n",
    "    * It is based on [Treebank tokenization](ftp://ftp.cis.upenn.edu/pub/treebank/public_html/tokenization.html) developed at UPenn.\n",
    "    * It splits on all whitespaces as well as contractions i.e. \"can't\" -> \"ca\", \"n't\"\n",
    "    * It tokenizes any consecutive number of punctuations, such as “,”, “?”, “—“, or “…”\n",
    "    * Punctuations inmixed with letters, such as “03/20/2018” would be tokenized as one word, as well as things like URL or hyphenated words like “open-faced”\n",
    "  * Stop words are not removed, as even simple terms like \"was\" and \"not\" appear at significantly different rates in positive and negative reviews.\n",
    "  * Only the top 5K terms by document frequency is retained. Terms with df less than 5 are also removed.\n",
    "  * TF-IDF weights are used for document-term matrix.\n",
    "1. **Multinomial naive Bayes** model is chosen\n",
    "  * Multinomial NB is chosen due to the training data being based on term counts, where the frequency of term matters as much as just apperence (the basis of Bernoulli naive Bayes model). \n",
    "  * Laplace smoothing is used with $\\alpha=1$, due to the large number of features and possibility of a term not appearing in the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize, # vectorize\n",
    "                             max_features=5000, min_df=5)),\n",
    "    ('clf', NB.MultinomialNB(alpha=1)) # classify\n",
    "])\n",
    "\n",
    "NB_tfidf.fit(train['text'], train['stars']) # fit model on training set\n",
    "pred_dev = NB_tfidf.predict(dev['text']) # pred based on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docMat = NB_tfidf.named_steps['vect'].transform(train['text'])\n",
    "tfidfTerms = NB_tfidf.named_steps['vect'].get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZSJnW6faaNFQoqq4ALqYg\t4\n",
      "Rcbv11hm5AYEwZyqYwAvg\t2\n",
      "rkRTjhu5szaBggeFVcVJlA\t4\n",
      "dhmeDsQGUS1FXMLs49SWjQ\t4\n",
      "z9zfIMYmRRCE4ggfOIieEw\t4\n",
      "Xtb3pGSh39bqcozkBECw\t2\n",
      "DOUflAGzxLsXG6xOmR1w\t2\n",
      "0RxCEWURe08CTcZt95F4AQ\t2\n",
      "MzUg5twEcCyd0X6lBMP2Lg\t2\n",
      "uNlw2D5CYKk0wjNxLtYw\t4\n"
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    print('%s\\t%d'%(dev['docID'][n],pred_dev[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Evaluate your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates various stats related to validation\n",
    "def validationStats(y_Prd, y_Act, msg='', algo='naive Bayes'):\n",
    "    # confusion matrix, T=true, F=false, N=negative, P=positive\n",
    "    TN, FP, FN, TP = sklearn.metrics.confusion_matrix(y_Act, y_Prd).ravel()\n",
    "    precision,recall = TP/(TP+FP) , TP/(TP+FN) # precision and recall\n",
    "    corr,tot = TN+TP , TN+TP+FN+FP # used for accuracy calculation\n",
    "    print(\"Using %s, %s\"%(algo,msg))\n",
    "    print(\"\\tTP=%d, TN=%d, FP=%d, FN=%d\"%(TP,TN,FP,FN))\n",
    "    print(\"\\tRecall: %u/%u = %.1f%%\" % (TP, TP+FN, recall*100) )\n",
    "    print(\"\\tPrecision: %u/%u = %.1f%%\" % (TP, TP+FP, precision*100) )\n",
    "    print(\"\\tF1 score: %.3f\" % (2*precision*recall / (precision+recall)) )\n",
    "    print(\"\\tAccuracy: %u/%u = %.1f%%\" % (corr,tot,corr/tot*100) )\n",
    "    return (TN, FP, FN, TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using naive Bayes, TF-IDF doc vectors\n",
      "\tTP=835, TN=838, FP=162, FN=165\n",
      "\tRecall: 835/1000 = 83.5%\n",
      "\tPrecision: 835/997 = 83.8%\n",
      "\tF1 score: 0.836\n",
      "\tAccuracy: 1673/2000 = 83.7%\n"
     ]
    }
   ],
   "source": [
    "validationStats(pred_dev, dev['stars'], 'TF-IDF doc vectors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "docMat = NB_tfidf.named_steps['vect'].transform(train['text'])\n",
    "tfidfTerms = NB_tfidf.named_steps['vect'].get_feature_names()\n",
    "\n",
    "def getTermCompByClass(termList):\n",
    "    idx = [tfidfTerms.index(x) for x in termList]\n",
    "    df = pd.DataFrame(docMat[:,idx].todense()).groupby(train['stars']).mean()\n",
    "    df.columns = termList\n",
    "    disp.display(disp.HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, I am not sure why people put down the Stratosphere, yea...yea the casino action kind of sucks....but don't go there to gamble, go there for Lucky's and Fat Tuesday's....But let's get back to Lucky's....we were here with a couple that had never been to the Strat, so we decided to go before having dinner, my honey and I decided to play some penny slot to kill time, and LO and BEHOLD...I saw a sign advertising steak and crab legs for 9.99, well, most people (NOT ME) would be scared off by that, but hell it was a hard night, I lost some moola and was looking for some cheap (but good) grub....I am after all a foodie...haha, only if you count greasy spoons. Anyways, back to Lucky's the dinner was really good, the steak was juicy, the crab legs meaty, tender and were already cut in half for you....shoot people, what more do you want, what more do you need.....Oh, but wait there's a catch, you can only order that between 7p-10p or 6-10, I forgot, but I do know it ends at 10p. Try to catch it, if you are cheap like me. A damn good, dinner, though, damn!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>not</th>\n",
       "      <th>sucks</th>\n",
       "      <th>but</th>\n",
       "      <th>n't</th>\n",
       "      <th>though</th>\n",
       "      <th>greasy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.041488</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.042361</td>\n",
       "      <td>0.042388</td>\n",
       "      <td>0.009895</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.019250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.024336</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.033935</td>\n",
       "      <td>0.027497</td>\n",
       "      <td>0.006978</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.017394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dev['text'][57])\n",
    "getTermCompByClass(['not', 'sucks', 'but', \"n't\", 'though', 'greasy', 'time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review was classified as a 4, but was actually a 2. The review starts out negatively, using words like \"not\", \"sucks\", \"but\", and \"don't\". However, the initial part of the review was describing the casino, not the restaurant itself. Depiste being a positive review, it's interspersed with words frequently found in negative reviews like \"though\", \"greasy\", and \"time\" (probably from people who waited a long time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has been about a month since we last visited this place.  I recommend going to the pub and not the restaraunt side.  Service was great.  We got a couple of pints and some wings.  Wings were overdone although the sauces were good.  Love the garlic parmesan wings, even though overdone.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommend</th>\n",
       "      <th>great</th>\n",
       "      <th>good</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.010960</td>\n",
       "      <td>0.025437</td>\n",
       "      <td>0.006714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008005</td>\n",
       "      <td>0.033682</td>\n",
       "      <td>0.041305</td>\n",
       "      <td>0.014412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dev['text'][974])\n",
    "getTermCompByClass(['recommend', 'great', 'good', 'love'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review was misclassified as a 4, but is actually a 2. Depiste the overall review being negative, it talks about positive aspects of the visit. The review contains words typically associated with positive reviews like \"recommend\", \"great\", \"good\", and \"love\". Therefore, it's easy to see what this was misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want atmosphere, it's a great, great, great coffee shop.  If you want espresso, food, or fast service, unfortunately, look elsewhere.    Every visit here has had me run into friendly, talkative, awesome people, but I go to a coffee shop wanting coffee, honestly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>great</th>\n",
       "      <th>friendly</th>\n",
       "      <th>awesome</th>\n",
       "      <th>fast</th>\n",
       "      <th>unfortunately</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010960</td>\n",
       "      <td>0.004824</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.005806</td>\n",
       "      <td>0.005428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033682</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>0.009526</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dev['text'][1437])\n",
    "getTermCompByClass(['great', 'friendly', 'awesome', 'fast', 'unfortunately'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review was misclassified as a 4. It is easy to see why this happened, as the reviewer used \"great\" three times as well as other words like \"good\", \"friendly\", and \"awesome\". The positive aspects of this review overwhelmed the use of negative words like \"unfortunately\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Build a second classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SVM, 5000 features\n",
      "\tTP=812, TN=825, FP=175, FN=188\n",
      "\tRecall: 812/1000 = 81.2%\n",
      "\tPrecision: 812/987 = 82.3%\n",
      "\tF1 score: 0.817\n",
      "\tAccuracy: 1637/2000 = 81.8%\n"
     ]
    }
   ],
   "source": [
    "SVM_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize, \n",
    "                             max_features=5000, min_df=2) ),\n",
    "    ('scl', sklearn.preprocessing.StandardScaler(copy=False, with_mean=False)),\n",
    "    ('clf', SVM.SVC(gamma='auto', max_iter=-1, random_state=1, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pred_dev2 = SVM_tfidf.fit(train['text'], train['stars']).predict(dev['text'])\n",
    "validationStats(pred_dev2, dev['stars'], '5000 features', 'SVM');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SVM, 2000 features\n",
      "\tTP=825, TN=865, FP=135, FN=175\n",
      "\tRecall: 825/1000 = 82.5%\n",
      "\tPrecision: 825/960 = 85.9%\n",
      "\tF1 score: 0.842\n",
      "\tAccuracy: 1690/2000 = 84.5%\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "SVM_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize, \n",
    "                             max_features=2000, min_df=2) ),\n",
    "    ('scl', sklearn.preprocessing.StandardScaler(copy=False, with_mean=False)),\n",
    "    ('clf', SVM.SVC(gamma='auto', max_iter=-1, random_state=1, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pred_dev3 = SVM_tfidf.fit(train['text'], train['stars']).predict(dev['text'])\n",
    "validationStats(pred_dev3, dev['stars'], '2000 features', 'SVM');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SVM, uni & bigram\n",
      "\tTP=853, TN=877, FP=123, FN=147\n",
      "\tRecall: 853/1000 = 85.3%\n",
      "\tPrecision: 853/976 = 87.4%\n",
      "\tF1 score: 0.863\n",
      "\tAccuracy: 1730/2000 = 86.5%\n"
     ]
    }
   ],
   "source": [
    "SVM_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize, ngram_range=(1,2),\n",
    "                             max_features=5000, min_df=2) ),\n",
    "    ('scl', sklearn.preprocessing.StandardScaler(copy=False, with_mean=False)),\n",
    "    ('clf', SVM.SVC(gamma='auto', max_iter=-1, random_state=1, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pred_dev4 = SVM_tfidf.fit(train['text'], train['stars']).predict(dev['text'])\n",
    "validationStats(pred_dev4, dev['stars'], 'uni & bigram', 'SVM');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SVM, bi & trigram\n",
      "\tTP=817, TN=831, FP=169, FN=183\n",
      "\tRecall: 817/1000 = 81.7%\n",
      "\tPrecision: 817/986 = 82.9%\n",
      "\tF1 score: 0.823\n",
      "\tAccuracy: 1648/2000 = 82.4%\n"
     ]
    }
   ],
   "source": [
    "SVM_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize, ngram_range=(2,3),\n",
    "                             max_features=5000, min_df=2) ),\n",
    "    ('scl', sklearn.preprocessing.StandardScaler(copy=False, with_mean=False)),\n",
    "    ('clf', SVM.SVC(gamma='auto', max_iter=-1, random_state=1, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pred_dev5 = SVM_tfidf.fit(train['text'], train['stars']).predict(dev['text'])\n",
    "validationStats(pred_dev5, dev['stars'], 'bi & trigram', 'SVM');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = SVM_tfidf.predict(test['text'])\n",
    "with open('jwu74.tsv', 'w') as fh:\n",
    "    for ID,star in zip(test['docID'], pred_test):\n",
    "        fh.write('%s\\t%d\\n'%(ID,star))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
