{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04\n",
    "\n",
    "student: John Wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, sys, csv, string, re, sklearn.preprocessing, sklearn.metrics\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn import naive_bayes as NB, svm as SVM\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "loading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file names\n",
    "trainFile = './data/train.tsv'\n",
    "testFile = './data/test.tsv'\n",
    "devFile = './data/dev.tsv'\n",
    "varNames = ['stars','docID','text']\n",
    "\n",
    "# read in files\n",
    "train = pd.read_csv(trainFile, sep='\\t', header=None, names=varNames)\n",
    "dev = pd.read_csv(devFile, sep='\\t', header=None, names=varNames)\n",
    "test = pd.read_csv(testFile, sep='\\t', header=None, names=varNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = re.compile('^['+string.punctuation+']+$') # match 1+ consec. punctuation\n",
    "# list of stop words in English, tokenized by word_tokenize()\n",
    "engStopWords =  set(nltk.word_tokenize( \\\n",
    "    ' '.join(nltk.corpus.stopwords.words('english')) ) ) \n",
    "\n",
    "def myTokenize(txt): # tokenize, no 1+ consec punct and stop words\n",
    "    return [tk for tk in nltk.word_tokenize(txt) if \\\n",
    "        (tk not in engStopWords and not punct.match(tk))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Study the training data\n",
    "\n",
    "This section explores the training data set to allow a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['stars'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The averge rating of the data set is 3, which means the training sample is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some idea of useful features for the data, we use `CountVectorizer` to count the number of term frequencies of terms appearing in each document. We set the CountVectorizer to only return binary counts (i.e. value=1 if term is in document at least once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "binVec = CountVectorizer(tokenizer=nltk.word_tokenize, binary=True)\n",
    "binTF = binVec.fit_transform(train['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative word frequency\n",
    "In the following section, we find the top terms with the biggest difference of document frequency between two and four-star reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great: 39.60% (pos), 17.80% (neg)\n",
      "was: 54.90% (pos), 75.60% (neg)\n",
      "not: 42.40% (pos), 62.00% (neg)\n",
      "!: 47.90% (pos), 28.40% (neg)\n",
      "were: 27.60% (pos), 41.60% (neg)\n",
      "n't: 45.30% (pos), 59.10% (neg)\n",
      "always: 22.70% (pos), 9.80% (neg)\n",
      "good: 55.70% (pos), 42.90% (neg)\n",
      "did: 14.40% (pos), 27.10% (neg)\n",
      "be: 32.60% (pos), 44.20% (neg)\n",
      "just: 26.70% (pos), 38.10% (neg)\n",
      "better: 11.70% (pos), 22.80% (neg)\n",
      "delicious: 14.40% (pos), 3.30% (neg)\n",
      "friendly: 17.50% (pos), 6.60% (neg)\n",
      "are: 45.00% (pos), 34.20% (neg)\n",
      "because: 13.80% (pos), 24.10% (neg)\n",
      "ordered: 13.90% (pos), 24.10% (neg)\n",
      "no: 15.50% (pos), 25.50% (neg)\n",
      "bad: 7.20% (pos), 17.00% (neg)\n",
      "at: 38.60% (pos), 47.10% (neg)\n"
     ]
    }
   ],
   "source": [
    "twoSt = (train['stars']==2).to_numpy() # idx for 2-star reviews\n",
    "tfDiff = np.abs(binTF[twoSt].mean(axis=0) - binTF[~twoSt].mean(axis=0))\n",
    "top20idx = tfDiff.A1.argsort()[-20:][::-1]\n",
    "terms = binVec.get_feature_names()\n",
    "for x in top20idx:\n",
    "    a,b = binTF[~twoSt,x].mean()*100, binTF[twoSt,x].mean()*100\n",
    "    print('%s: %.2f%% (pos), %.2f%% (neg)'%(terms[x],a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms are listed in order of disparity. As expected, words like \"great\", \"always\", \"delicious\", and \"friendly\" are expected to have a high presence in positive reviews, and \"bad\" has a high presence in negative reviews. However, there are also some counter-intuitive examples. The term \"better\" is more frequently seen in negative reviews due to expressions like \"there are many others in Charlotte that are better\" and \"maybe the next time I come in the food will be better\". While the term \"like\" can connotate a favorable feeling, it is also used in simile, which are present in negative reviews such as \"it tastes like a combo of cream cheese, american cheese and sour cream\". \n",
    "\n",
    "Another unintuitive term to have disparity is \"ordered\". After looking through negative reviews, they often contain details which list the items ordered and how they are bad, such as \"my friend ordered a virgin strawberry daiquiri and instead she got some weird smoothie with whip cream on top\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other useful characteristics\n",
    "\n",
    "In this section, we explore a few characteristics that are different between the two types of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "2    720.375\n",
       "4    631.283\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textLen = train['text'].str.len() # length of text\n",
    "textLen.groupby(train['stars']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative reviews are 90 characters longer on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "2    0.025294\n",
       "4    0.027273\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capPct = train['text'].str.count(r'[A-Z]')/textLen # % of chars upper case\n",
    "capPct.groupby(train['stars']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive reviews tend to have a slightly larger proportion of upper case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "2    0.000543\n",
       "4    0.000399\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nPunct = train['text'].str.count('2')\n",
    "(nPunct/textLen).groupby(train['stars']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-star reviews tend to have more mentions of the number \"2\", likely from the reviews explicitly enumerating the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "NB_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize,\n",
    "                            max_features=5000, min_df=5)),\n",
    "    ('clf', NB.MultinomialNB(alpha=1))\n",
    "])\n",
    "\n",
    "NB_tfidf.fit(train['text'], train['stars']) # fit model on training set\n",
    "pred_dev = NB_tfidf.predict(dev['text']) # pred based on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZSJnW6faaNFQoqq4ALqYg\t4\n",
      "Rcbv11hm5AYEwZyqYwAvg\t2\n",
      "rkRTjhu5szaBggeFVcVJlA\t4\n",
      "dhmeDsQGUS1FXMLs49SWjQ\t4\n",
      "z9zfIMYmRRCE4ggfOIieEw\t4\n",
      "Xtb3pGSh39bqcozkBECw\t2\n",
      "DOUflAGzxLsXG6xOmR1w\t2\n",
      "0RxCEWURe08CTcZt95F4AQ\t2\n",
      "MzUg5twEcCyd0X6lBMP2Lg\t2\n",
      "uNlw2D5CYKk0wjNxLtYw\t4\n"
     ]
    }
   ],
   "source": [
    "for n in range(10):\n",
    "    print('%s\\t%d'%(dev['docID'][n],pred_dev[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Evaluate your predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates various stats related to validation\n",
    "def validationStats(y_Prd, y_Act, msg='', algo='naive Bayes'):\n",
    "    # confusion matrix, T=true, F=false, N=negative, P=positive\n",
    "    TN, FP, FN, TP = sklearn.metrics.confusion_matrix(y_Act, y_Prd).ravel()\n",
    "    precision,recall = TP/(TP+FP) , TP/(TP+FN) # precision and recall\n",
    "    corr,tot = TN+TP , TN+TP+FN+FP # used for accuracy calculation\n",
    "    print(\"Using %s, %s\"%(algo,msg))\n",
    "    print(\"\\tTP=%d, TN=%d, FP=%d, FN=%d\"%(TP,TN,FP,FN))\n",
    "    print(\"\\tRecall: %u/%u = %.1f%%\" % (TP, TP+FN, recall*100) )\n",
    "    print(\"\\tPrecision: %u/%u = %.1f%%\" % (TP, TP+FP, precision*100) )\n",
    "    print(\"\\tF1 score: %.3f\" % (2*precision*recall / (precision+recall)) )\n",
    "    print(\"\\tAccuracy: %u/%u = %.1f%%\" % (corr,tot,corr/tot*100) )\n",
    "    return (TN, FP, FN, TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using naive Bayes, TF-IDF doc vectors\n",
      "\tTP=835, TN=838, FP=162, FN=165\n",
      "\tRecall: 835/1000 = 83.5%\n",
      "\tPrecision: 835/997 = 83.8%\n",
      "\tF1 score: 0.836\n",
      "\tAccuracy: 1673/2000 = 83.7%\n"
     ]
    }
   ],
   "source": [
    "validationStats(pred_dev, dev['stars'], 'TF-IDF doc vectors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "2    0.057\n",
       "4    0.087\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(binTF[:,terms.index('fast')].todense().A1).groupby(train['stars']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ok, I am not sure why people put down the Stratosphere, yea...yea the casino action kind of sucks....but don't go there to gamble, go there for Lucky's and Fat Tuesday's....But let's get back to Lucky's....we were here with a couple that had never been to the Strat, so we decided to go before having dinner, my honey and I decided to play some penny slot to kill time, and LO and BEHOLD...I saw a sign advertising steak and crab legs for 9.99, well, most people (NOT ME) would be scared off by that, but hell it was a hard night, I lost some moola and was looking for some cheap (but good) grub....I am after all a foodie...haha, only if you count greasy spoons. Anyways, back to Lucky's the dinner was really good, the steak was juicy, the crab legs meaty, tender and were already cut in half for you....shoot people, what more do you want, what more do you need.....Oh, but wait there's a catch, you can only order that between 7p-10p or 6-10, I forgot, but I do know it ends at 10p. Try to catch it, if you are cheap like me. A damn good, dinner, though, damn!\""
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['text'][57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review was classified as a 4, but was actually a 2. The review starts out negatively, using words like \"not\", \"sucks\", \"but\", and \"don't\". However, the initial part of the review was describing the casino, not the restaurant itself. Depiste being a positive review, it's interspersed with words frequently found in negative reviews like \"though\", \"greasy\", and \"time\" (probably from people who waited a long time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It has been about a month since we last visited this place.  I recommend going to the pub and not the restaraunt side.  Service was great.  We got a couple of pints and some wings.  Wings were overdone although the sauces were good.  Love the garlic parmesan wings, even though overdone.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['text'][974]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review was misclassified as a 4, but is actually a 2. Depiste the overall review being negative, it talks about positive aspects of the visit. The review contains words typically associated with positive reviews like \"recommend\", \"great\", \"good\", and \"love\". Therefore, it's easy to see what this was misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you want atmosphere, it's a great, great, great coffee shop.  If you want espresso, food, or fast service, unfortunately, look elsewhere.    Every visit here has had me run into friendly, talkative, awesome people, but I go to a coffee shop wanting coffee, honestly.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['text'][1437]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review was misclassified as a 4. It is easy to see why this happened, as the reviewer used \"great\" three times as well as other words like \"good\", \"friendly\", and \"awesome\". The positive aspects of this review overwhelmed the use of negative words like \"unfortunately\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Build a second classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SVM, 5000 features\n",
      "\tTP=812, TN=825, FP=175, FN=188\n",
      "\tRecall: 812/1000 = 81.2%\n",
      "\tPrecision: 812/987 = 82.3%\n",
      "\tF1 score: 0.817\n",
      "\tAccuracy: 1637/2000 = 81.8%\n"
     ]
    }
   ],
   "source": [
    "SVM_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize, \n",
    "                             max_features=5000, min_df=2) ),\n",
    "    ('scl', sklearn.preprocessing.StandardScaler(copy=False, with_mean=False)),\n",
    "    ('clf', SVM.SVC(gamma='auto', max_iter=-1, random_state=1, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pred_dev2 = SVM_tfidf.fit(train['text'], train['stars']).predict(dev['text'])\n",
    "validationStats(pred_dev2, dev['stars'], '5000 features', 'SVM');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SVM, 2000 features\n",
      "\tTP=825, TN=865, FP=135, FN=175\n",
      "\tRecall: 825/1000 = 82.5%\n",
      "\tPrecision: 825/960 = 85.9%\n",
      "\tF1 score: 0.842\n",
      "\tAccuracy: 1690/2000 = 84.5%\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "SVM_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize, \n",
    "                             max_features=2000, min_df=2) ),\n",
    "    ('scl', sklearn.preprocessing.StandardScaler(copy=False, with_mean=False)),\n",
    "    ('clf', SVM.SVC(gamma='auto', max_iter=-1, random_state=1, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pred_dev3 = SVM_tfidf.fit(train['text'], train['stars']).predict(dev['text'])\n",
    "validationStats(pred_dev3, dev['stars'], '2000 features', 'SVM');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SVM, uni & bigram\n",
      "\tTP=853, TN=877, FP=123, FN=147\n",
      "\tRecall: 853/1000 = 85.3%\n",
      "\tPrecision: 853/976 = 87.4%\n",
      "\tF1 score: 0.863\n",
      "\tAccuracy: 1730/2000 = 86.5%\n"
     ]
    }
   ],
   "source": [
    "SVM_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize, ngram_range=(1,2),\n",
    "                             max_features=5000, min_df=2) ),\n",
    "    ('scl', sklearn.preprocessing.StandardScaler(copy=False, with_mean=False)),\n",
    "    ('clf', SVM.SVC(gamma='auto', max_iter=-1, random_state=1, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pred_dev4 = SVM_tfidf.fit(train['text'], train['stars']).predict(dev['text'])\n",
    "validationStats(pred_dev4, dev['stars'], 'uni & bigram', 'SVM');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SVM, bi & trigram\n",
      "\tTP=817, TN=831, FP=169, FN=183\n",
      "\tRecall: 817/1000 = 81.7%\n",
      "\tPrecision: 817/986 = 82.9%\n",
      "\tF1 score: 0.823\n",
      "\tAccuracy: 1648/2000 = 82.4%\n"
     ]
    }
   ],
   "source": [
    "SVM_tfidf = Pipeline([ # establish pipeline\n",
    "    ('vect', TfidfVectorizer(tokenizer=nltk.word_tokenize, ngram_range=(2,3),\n",
    "                             max_features=5000, min_df=2) ),\n",
    "    ('scl', sklearn.preprocessing.StandardScaler(copy=False, with_mean=False)),\n",
    "    ('clf', SVM.SVC(gamma='auto', max_iter=-1, random_state=1, kernel='rbf'))\n",
    "])\n",
    "\n",
    "pred_dev5 = SVM_tfidf.fit(train['text'], train['stars']).predict(dev['text'])\n",
    "validationStats(pred_dev5, dev['stars'], 'bi & trigram', 'SVM');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = SVM_tfidf.predict(test['text'])\n",
    "with open('jwu74.tsv', 'w') as fh:\n",
    "    for ID,star in zip(test['docID'], pred_test):\n",
    "        fh.write('%s\\t%d\\n'%(ID,star))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jwu)",
   "language": "python",
   "name": "conda_jwu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
