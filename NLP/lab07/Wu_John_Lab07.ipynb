{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, re\n",
    "from dateutil.parser import parse as dateParser\n",
    "pprint.sorted = lambda x, key=None: x # disable sorting of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\") # load spacy English model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: Information Extraction\n",
    "\n",
    "Student: John Wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagCap = re.compile(r'<P ID=(\\d+)>\\s+(.+?)\\s?</P>', re.DOTALL)\n",
    "def readFiles(filePath):\n",
    "    with open(filePath, 'r', encoding='utf-8') as fh:\n",
    "        matches = tagCap.findall(fh.read())\n",
    "        ids,txts = zip(*matches)\n",
    "        ids = [int(s) for s in ids]\n",
    "        return ids, txts\n",
    "    \n",
    "trIDs, trTxts = readFiles('data/obits.train.txt') # get training files\n",
    "\n",
    "# def firstTknAfterChar(doc, charOffset):\n",
    "#     if charOffset >= len(doc.text): # if charOffset is after end of doc\n",
    "#         return None\n",
    "#     for tk in doc: # loop over tokens, find 1st token after offset\n",
    "#         if tk.idx >= charOffset: # if token char-idx after offset\n",
    "#             return tk # return token (contains char-idx and tkn-idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Required Relations\n",
    "\n",
    "__Name of the deceased__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractName(doc):\n",
    "    sent = next(doc.sents) # get first sentence of document\n",
    "    for n,tk in enumerate(sent): # loop over tokens of first sentence\n",
    "        if (tk.is_digit or tk.shape_[0]=='x' or tk.is_punct and \n",
    "                not (tk.text==',' or tk.is_left_punct or tk.is_right_punct)):\n",
    "            break\n",
    "    if doc[n-1].is_punct: # if span ends with punctuation, reduce span by 1\n",
    "        n -= 1\n",
    "    nameSpan = doc[:n]\n",
    "    \n",
    "    # see if any PERSON entity was matched in span\n",
    "    if 'PERSON' not in set((tk.ent_type_ for tk in nameSpan)): # no PERSON\n",
    "        for e in doc.ents: # loop over all extracted entities\n",
    "            if e.label_ == 'PERSON': # find first PERSON entity\n",
    "                nameSpan = e # set that entity to the name\n",
    "                break\n",
    "    return nameSpan.text, nameSpan.end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sex of the deceased__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isMan(tokens):\n",
    "    tokens = [tk.lower_ for tk in tokens]\n",
    "    female = sum((tk=='her' or tk=='she' for tk in tokens))\n",
    "    male = sum((tk=='his' or tk=='he' for tk in tokens))\n",
    "    return male >= female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Age at death__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ageMatch = spacy.matcher.Matcher(nlp.vocab)\n",
    "ageMatch.add(\"age\", None, [{\"TEXT\": {\"REGEX\": \"^(1?\\d\\d)$\"}}])\n",
    "def getAgeDoc(doc, nameEnd=None):\n",
    "    matches = ageMatch(doc)\n",
    "    spanEnd = nameEnd if nameEnd else next(doc.sents).end\n",
    "\n",
    "    numPos = list()\n",
    "    for n,(nx,mBeg,mEnd) in enumerate(matches):\n",
    "        if 0 < mBeg < spanEnd:\n",
    "            return int(doc[mBeg:mEnd].text),mBeg\n",
    "        if mBeg <= len(doc)//2 and doc[mBeg].ent_type_ != 'DATE':\n",
    "            numPos.append( (int(doc[mBeg:mEnd].text),mBeg) )\n",
    "    return max(numPos, key=lambda x: x[0]) if numPos else (None,None)\n",
    "\n",
    "getAgeDoc(nlp(trTxts[15]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Location(s) of residency__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Plymouth, Massachusetts', 'Sanibel', 'the Gulf of Mexico'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## note that first half of document likely contains most of the locations where\n",
    "## the deceased live. The second half more likely to contain locations where\n",
    "## the death happened and where the funeral service is held, which may not be\n",
    "## the living location of the deceased.\n",
    "###############################################################################\n",
    "\n",
    "survivor = spacy.matcher.Matcher(nlp.vocab)\n",
    "survivor.add(\"surv\", None, [{\"LOWER\":{\"REGEX\": r'(surviv|pre-?deceas).*'}}])\n",
    "locLikes = {'GPE', 'LOC', 'FAC', 'ORG', 'NORP'}\n",
    "\n",
    "def findLocations(doc):\n",
    "    part = doc[:len(doc)//10*8] # exclude the end (where funeral info are)\n",
    "    locs,locIdxs = set(),set()\n",
    "    \n",
    "    for s in [doc[m[1]].sent for m in survivor(doc)]: # loop over sentences\n",
    "        locIdxs.update( range(s.start,s.end+1) )\n",
    "    \n",
    "    for loc in (e for e in part.ents if (e.label_=='GPE' or e.label_=='LOC')):\n",
    "        if loc.start in locIdxs:\n",
    "            continue\n",
    "        end = loc.end\n",
    "        while ((doc[end].text==',' and doc[end+1].ent_type_ in locLikes) or \n",
    "               doc[end].ent_type_ in locLikes):\n",
    "            end += 1\n",
    "        locs.add(doc[loc.start:end].text)\n",
    "        locIdxs.update( range(loc.start,end) )\n",
    "    return locs # cannot find any GPE entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.displacy.render(nlp(trTxts[6]), style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Spouse(s) of the deceased__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns like: survivors including his wife..., greatly missed by her husband...,\n",
    "# predeceased by ...\n",
    "# since many obits describe deceased as great husband/wife, must use gender to \n",
    "# distinguish descriptor of the deceased vs the spouse. Also gender is easy to find\n",
    "\n",
    "spouses = spacy.matcher.Matcher(nlp.vocab)\n",
    "spouseRe = r'(husband|wife|spouse|partner|married).*'\n",
    "spouses.add('sp', None, [{\"LOWER\": {\"REGEX\":spouseRe}}] )\n",
    "\n",
    "def findSpouseName(doc):\n",
    "    matches = spouses(doc) # search for word related to spouses\n",
    "    if not matches: # if no match, assumes no spouse can be found\n",
    "        return None\n",
    "    \n",
    "    for x,mtBeg,mtEnd in matches:\n",
    "        span = doc[mtBeg: doc[mtBeg].sent.end]\n",
    "        for et in span.ents: # loop over all persons in text span\n",
    "            if et.label_ == 'PERSON': \n",
    "                return et.text # return the first person found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Additional Relations\n",
    "\n",
    "Three additional relations extracted from the text (if available) were: date of birth, date of death, and date of a funeral service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thursday, March 28, 2019'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdayMatch = spacy.matcher.Matcher(nlp.vocab)\n",
    "bdayMatch.add(\"bday\", None, [{\"LOWER\":\"born\"}],[{\"LOWER\":\"birth\"}])\n",
    "\n",
    "deathMatch = spacy.matcher.Matcher(nlp.vocab)\n",
    "deathSyns = r'^(die|pass|.?sleep|heaven|succumb|perish).*' # synonyms for death\n",
    "deathMatch.add(\"death\", None, [{\"LOWER\":{\"REGEX\": deathSyns}}])\n",
    "\n",
    "funeralMatch = spacy.matcher.Matcher(nlp.vocab)\n",
    "funeralMatch.add(\"celebration\", None, [{\"LOWER\":\"life\"},{\"LOWER\":\"celebration\"}])\n",
    "funeralMatch.add(\"serv\", None, [{\"LOWER\":{\"REGEX\": r'^(service|visitation)s?'}}]) \n",
    "funeralMatch.add(\"memo\", None, [{\"LOWER\":\"memorial\"}], [{\"LOWER\":\"viewing\"}])\n",
    "\n",
    "################################################################################\n",
    "def findDateAfterMatch(doc, matcher, startAtMatch=False):\n",
    "    matched = matcher(doc)\n",
    "    if not matched: # if no match, then nothing is after\n",
    "        return None\n",
    "    matchedSent = doc[matched[0][1]].sent # assume 1st match is good\n",
    "    spanBeg = matched[0][1] if startAtMatch else matchedSent.start\n",
    "    spanEnd = matchedSent.end\n",
    "    for n in range(2):\n",
    "        if spanEnd >= len(doc): # if no more sentences left in doc\n",
    "            break\n",
    "        spanEnd = doc[spanEnd].sent.end\n",
    "        \n",
    "    for et in doc[spanBeg:spanEnd].ents:\n",
    "        if et.label_ == 'DATE' and re.findall(r'\\d\\d', et.text):\n",
    "            return et.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Template and Outputting Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTemplate(doc):\n",
    "    info = dict()\n",
    "    info['name'],nameEnd = extractName(doc)\n",
    "    info['sex'] = 'male' if isMan(doc) else 'female'\n",
    "    info['age'] = getAgeDoc(doc)[0] # get age using document parsing\n",
    "    info['locations'] = list(findLocations(doc))\n",
    "    info['spouse'] = findSpouseName(doc)\n",
    "    info['birth date'] = findDateAfterMatch(doc, bdayMatch)\n",
    "    info['death date'] = findDateAfterMatch(doc, deathMatch, startAtMatch=True)\n",
    "    info['funeral date'] = findDateAfterMatch(doc, funeralMatch, True)\n",
    "################################################################################\n",
    "    # try to calculate age at death using birth and death (or funeral) dates\n",
    "    if info['birth date'] and (info['death date'] or info['funeral date']):\n",
    "        bd = dateParser(info['birth date']) # parse birth date\n",
    "        if info['death date']: # if death date is stated, parse death date\n",
    "            dd = dateParser(info['death date']) \n",
    "        else: # otherwise use funeral service date as proxy for death\n",
    "            dd = dateParser(info['funeral date'])\n",
    "        elapsedYrs = (dd.year-bd.year) - ((dd.month,dd.day)<(bd.month,bd.day))\n",
    "        if not info['age'] or abs(info['age'] - elapsedYrs)>2:\n",
    "            info['age'] = elapsedYrs            \n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseObitsOutputInfo(obitFiles, outInfoFiles):\n",
    "    docIDs,txts = readFiles(obitFiles)\n",
    "    \n",
    "    with open(outInfoFiles, 'w', encoding='utf-8') as outFH:\n",
    "        for docID,txt in zip(docIDs,txts):\n",
    "            doc = nlp(txt)\n",
    "            out = {'ID': docID}\n",
    "            out.update(fillTemplate(doc))\n",
    "            pprint.pprint(out, outFH)\n",
    "            outFH.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseObitsOutputInfo('data/obits.train.txt', 'obits.train2.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parseObitsOutputInfo('data/obits.test.txt', 'obits.test.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateutil.parser.parse('Sunday, January 13, 2019') >= dateutil.parser.parse('January 04, 2019')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
