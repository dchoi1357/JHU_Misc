{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, re, dateutil.parser\n",
    "pprint.sorted = lambda x, key=None: x # disable sorting of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\") # load spacy English model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: Information Extraction\n",
    "\n",
    "Student: John Wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagCap = re.compile(r'<P ID=(\\d+)>\\s+(.+?)\\s?</P>', re.DOTALL)\n",
    "def readFiles(filePath):\n",
    "    with open(filePath, 'r', encoding='utf-8') as fh:\n",
    "        matches = tagCap.findall(fh.read())\n",
    "        ids,txts = zip(*matches)\n",
    "        ids = [int(s) for s in ids]\n",
    "        return ids, txts\n",
    "    \n",
    "trIDs, trTxts = readFiles('data/obits.train.txt') # get training files\n",
    "\n",
    "def firstTknAfterChar(doc, charOffset):\n",
    "    if charOffset >= len(doc.text): # if charOffset is after end of doc\n",
    "        return None\n",
    "    for tk in doc: # loop over tokens, find 1st token after offset\n",
    "        if tk.idx >= charOffset: # if token char-idx after offset\n",
    "            return tk # return token (contains char-idx and tkn-idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Required Relations\n",
    "\n",
    "__Name of the deceased__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractName(doc):\n",
    "    sent = next(doc.sents) # get first sentence of document\n",
    "    for n,tk in enumerate(sent): # loop over tokens of first sentence\n",
    "        if (tk.is_digit or tk.shape_[0]=='x' or tk.is_punct and \n",
    "                not (tk.text==',' or tk.is_left_punct or tk.is_right_punct)):\n",
    "            break\n",
    "    if doc[n-1].is_punct: # if span ends with punctuation, reduce span by 1\n",
    "        n -= 1\n",
    "    nameSpan = doc[:n]\n",
    "    \n",
    "    # see if any PERSON entity was matched in span\n",
    "    if 'PERSON' not in set((tk.ent_type_ for tk in nameSpan)): # no PERSON\n",
    "        for e in doc.ents: # loop over all extracted entities\n",
    "            if e.label_ == 'PERSON': # find first PERSON entity\n",
    "                nameSpan = e # set that entity to the name\n",
    "                break\n",
    "    return nameSpan.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sex of the deceased__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isMan(tokens):\n",
    "    tokens = [tk.lower_ for tk in tokens]\n",
    "    female = sum((tk=='her' or tk=='she' for tk in tokens))\n",
    "    male = sum((tk=='his' or tk=='he' for tk in tokens))\n",
    "    return male >= female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Age at death__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ageMatch = spacy.matcher.Matcher(nlp.vocab)\n",
    "ageMatch.add(\"age\", None, [{\"TEXT\": {\"REGEX\": \"^(1?\\d\\d)$\"}}])\n",
    "def getAgeDoc(doc):\n",
    "    matches = ageMatch(doc)\n",
    "    numPos = [(int(doc[m[1]:m[2]].text),m[1]) for m in matches]\n",
    "    return max(numPos, key=lambda x: x[0]) if matches else (None,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Location(s) of residency__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note that first half of document likely contains most of the locations where\n",
    "## the deceased live. The second half more likely to contain locations where\n",
    "## the death happened and where the funeral service is held, which may not be\n",
    "## the living location of the deceased.\n",
    "###############################################################################\n",
    "\n",
    "survivor = spacy.matcher.Matcher(nlp.vocab)\n",
    "survivor.add(\"surv\", None, [{\"LOWER\":{\"REGEX\": r'(surviv|pre-?deceas).*'}}])\n",
    "\n",
    "def findLocations(doc):\n",
    "    part = doc[:len(doc)//10*8] # exclude the end (where funeral info are)\n",
    "    locs,locIdxs = set(),set()\n",
    "    \n",
    "    for s in [doc[m[1]].sent for m in survivor(doc)]: # loop over sentences\n",
    "        locIdxs.update( range(s.start,s.end+1) )\n",
    "    \n",
    "    for loc in (e for e in part.ents if (e.label_=='GPE' or e.label_=='LOC')):\n",
    "        if loc.start in locIdxs:\n",
    "            continue\n",
    "        endIdx = loc.end\n",
    "        while ((doc[endIdx].text==',' and doc[endIdx+1].ent_type_) or \n",
    "               doc[endIdx].ent_type_):\n",
    "            endIdx += 1\n",
    "        locs.add(doc[loc.start:endIdx].text)\n",
    "        locIdxs.update( range(loc.start,endIdx) )\n",
    "    return locs # cannot find any GPE entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.displacy.render(nlp(trTxts[19]), style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Spouse(s) of the deceased__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spouses(nlp(trTxts[11]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns like: survivors including his wife..., greatly missed by her husband...,\n",
    "# predeceased by ...\n",
    "# since many obits describe deceased as great husband/wife, must use gender to \n",
    "# distinguish descriptor of the deceased vs the spouse. Also gender is easy to find\n",
    "\n",
    "spouses = spacy.matcher.Matcher(nlp.vocab)\n",
    "spouses.add('sp', None, [{\"LOWER\":{\"REGEX\":r'(husband|wife|spouse|partner).*'}}])\n",
    "\n",
    "def findSpouseName(doc):\n",
    "    mt = spouses(doc) # search for word related to spouses\n",
    "    if not mt: # if no match, assumes no spouse can be found\n",
    "        return None\n",
    "    else: # if found, choose rest of sentence as the document span\n",
    "        span = doc[mt[0][1]: doc[mt[0][1]].sent.end]\n",
    "    \n",
    "    for et in span.ents: # loop over all persons in text span\n",
    "        if et.label_ == 'PERSON': \n",
    "            return et.text # return the first person found\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Additional Relations\n",
    "\n",
    "Two additional relations extracted were: date of birth and date of death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdayMatch = spacy.matcher.Matcher(nlp.vocab)\n",
    "bdayMatch.add(\"bday\", None, [{\"LOWER\":\"born\"}],[{\"LOWER\":\"birth\"}])\n",
    "\n",
    "deathMatch = spacy.matcher.Matcher(nlp.vocab)\n",
    "deathSyns = r'^(die|pass|.?sleep|heaven|succumb|perish).*' # synonyms for death\n",
    "deathMatch.add(\"death\", None, [{\"LOWER\":{\"REGEX\": deathSyns}}])\n",
    "\n",
    "################################################################################\n",
    "def findDateAfterMatch(doc, matcher, startAtMatch=False):\n",
    "    matched = matcher(doc)\n",
    "    if not matched: # if no match, then nothing is after\n",
    "        return None\n",
    "    matchedSent = doc[matched[0][1]].sent # assume 1st match is good\n",
    "    spanBeg = matched[0][1] if startAtMatch else matchedSent.start\n",
    "    spanEnd = matchedSent.end\n",
    "    for n in range(2):\n",
    "        spanEnd = doc[spanEnd].sent.end\n",
    "        \n",
    "    for et in doc[spanBeg:spanEnd].ents:\n",
    "        if et.label_ == 'DATE' and re.findall(r'\\d\\d', et.text):\n",
    "            return et.text\n",
    "    return None\n",
    "\n",
    "# findDateAfterMatch(nlp(trTxts[11]), deathMatch, startAtMatch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Template and Outputting Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillTemplate(doc):\n",
    "    info = dict()\n",
    "    info['name'] = extractName(doc)\n",
    "    info['sex'] = 'male' if isMan(doc) else 'female'\n",
    "    info['age'] = getAgeDoc(doc)[0] # get age using document parsing\n",
    "    info['locations'] = list(findLocations(doc))\n",
    "    info['spouse'] = findSpouseName(doc)\n",
    "    info['birth date'] = findDateAfterMatch(doc, bdayMatch)\n",
    "    info['death date'] = findDateAfterMatch(doc, deathMatch, startAtMatch=True)\n",
    "    \n",
    "    if info['birth date'] and info['death date']: # if neither date is missing\n",
    "        bd = dateutil.parser.parse(info['birth date']) # parse datetime for both\n",
    "        dd = dateutil.parser.parse(info['death date']) # birthday and death day\n",
    "        info['age'] = (dd.year-bd.year) - ((dd.month,dd.day)<(bd.month,bd.day))\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseObitsOutputInfo(obitFiles, outInfoFiles):\n",
    "    docIDs,txts = readFiles(obitFiles)\n",
    "    \n",
    "    with open(outInfoFiles, 'w') as outFH:\n",
    "        for docID,txt in zip(docIDs,txts):\n",
    "            doc = nlp(txt)\n",
    "            out = {'ID': docID}\n",
    "            out.update(fillTemplate(doc))\n",
    "            pprint.pprint(out, outFH)\n",
    "            outFH.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseObitsOutputInfo('data/obits.train.txt', 'obits.train.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parseObitsOutputInfo('data/obits.test.txt', 'obits.test.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jwu)",
   "language": "python",
   "name": "conda_jwu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
