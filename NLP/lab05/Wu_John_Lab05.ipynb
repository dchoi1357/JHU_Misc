{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05\n",
    "\n",
    "Student: John Wu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, sys, gensim, unicodedata\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of small.txt file\n",
    "\n",
    "The file contains sentences delimited by line breaks. However, the sentences need to be tokenized first before it can be processed. To do this, we use the [Tok-Tok](https://github.com/jonsafari/tok-tok) tokenizer, which is fast (though simple) tokenizer of sentences that work on different Romance languages.\n",
    "\n",
    "Since we need to remove puncutations as well, a function is implemented to check if the token is of length 1, and if it is, the [unicode category](https://unicodebook.readthedocs.io/unicode.html#categories) of the token is of punctuation or symbol (or `P` and `S`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctCatgs = {'P', 'S'}\n",
    "def isPunct(tkn): # if length=1 and in unicode punctuation and symbol cateogry\n",
    "    return len(tkn) == 1 and unicodedata.category(tkn)[0] in punctCatgs\n",
    "\n",
    "toktok = nltk.toktok.ToktokTokenizer() # instantiate TokTok tokenizer\n",
    "\n",
    "def tokSentNoPunc(txt): # sentence tokenizer\n",
    "    return [t for t in toktok.tokenize(txt) if not isPunct(t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is read in as unicode since there are presence of non-ASCII punctuations and letters. The code below reads the file and print out a sample of the tokenized result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFileAndTokenize(filePath):\n",
    "    with open(filePath, 'r', encoding='utf-8') as f:    \n",
    "        return [tokSentNoPunc(l.casefold()) for l in f] # read lines and tokenize\n",
    "\n",
    "sents = readFileAndTokenize('./data/small.txt')\n",
    "print(sents[100:104])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a word2vec model__\n",
    "\n",
    "Using the gensim library, a 100-dimension word2vec model is built and word embeddings calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = Word2Vec(sents, size=100, window=5, min_count=2, sg=1, negative=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a visualization of embedding vectors, we get the first two principal components of several terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrds = ['paris', 'istanbul', 'moscow', 'france', 'turkey', 'russia', 'cat', \n",
    "        'dog', 'truck', 'train', 'two', 'three', 'four']\n",
    "v = np.vstack([mdl.wv[s] for s in wrds]).T # stack vectors from all words\n",
    "pc2 = PCA(n_components=2) # model for PC1 and PC2\n",
    "pc2.fit(v); # get the PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the PC1 on x-axis, PC2 on y-axis, along with text annotation of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pc2.components_[0], pc2.components_[1])\n",
    "for n,w in enumerate(wrds):\n",
    "    plt.text(pc2.components_[0,n], pc2.components_[1,n], w)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___PUT IN ANALYSIS HERE___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google News Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googFile = './data/GoogleNews-vectors-negative300.bin'\n",
    "googMdl = KeyedVectors.load_word2vec_format(googFile, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googMdl.most_similar('fascinating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googMdl.most_similar('cultivate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googMdl.distances('Vietnam', ['Spain', 'China', 'Egypt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googMdl.distances('mother', ['father', 'teacher', 'ocean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googMdl.most_similar_cosmul(['puppies', 'cat'], ['dog'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googMdl.most_similar_cosmul(['read', 'music'], ['book'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googMdl.most_similar_cosmul(['hot', 'winter'], ['summer'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googMdl.distances('small', ['tiny', 'large', 'ice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del googMdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding for Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in text files for both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engSents = readFileAndTokenize('./data/eng.txt')\n",
    "spaSents = readFileAndTokenize('./data/spa.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models and saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engModel = Word2Vec(engSents, size=100, window=5, min_count=2, sg=1, negative=8)\n",
    "spaModel = Word2Vec(spaSents, size=100, window=5, min_count=2, sg=1, negative=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engModel.wv.save_word2vec_format('eng.w2v.model')\n",
    "spaModel.wv.save_word2vec_format('spa.w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run vecmap code to learn bilingual projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./vecmap/map_embeddings.py --supervised data/es-en.train.txt spa.w2v.model eng.w2v.model \\\n",
    "    spa_mapped.emb eng_mapped.emb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the bilingual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa2eng = gensim.models.KeyedVectors.load_word2vec_format('spa_mapped.emb')\n",
    "eng2spa = gensim.models.KeyedVectors.load_word2vec_format('eng_mapped.emb') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the various translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSpaToEng(words):\n",
    "    if isinstance(words, str):\n",
    "        words = [words]\n",
    "    for w in words:\n",
    "        trans = eng2spa.similar_by_vector(spa2eng[w])[0]\n",
    "        print(\"Sp: %s = En: %s, (%f)\" % (w, trans[0], trans[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['playa', 'villa', 'perros', 'naufragio', 'islas', 'cantar', \n",
    "     'calles', 'naranjas', 'bomberos', 'escalera', 'nadó','frontera',\n",
    "     'pasaporte', 'fábrica']\n",
    "printSpaToEng(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = ['jugar', 'juego', 'juegas', 'juega', 'jugamos', 'juegan']\n",
    "printSpaToEng(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3 = ['europeo', 'español', 'cubano', 'ecuatoriano', 'francés', 'alemán', \n",
    "      'chino', 'japonés', 'americano', 'estadounidense',  'egipcio',\n",
    "      'turco', 'nigeriano']\n",
    "printSpaToEng(l3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jwu)",
   "language": "python",
   "name": "conda_jwu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
