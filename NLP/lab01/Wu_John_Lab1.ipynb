{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1\n",
    "Student: **John Wu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in files. The tokens.txt is read in as one blob of text. The sentences.txt is read in as a list where each element is each line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentFile = './data/sentences.txt'\n",
    "tokFile = './data/tokens.txt'\n",
    "\n",
    "with open(tokFile) as f:\n",
    "    tokBlob = f.read() # read entire blob\n",
    "\n",
    "with open(sentFile) as f:\n",
    "    sentLines = f.read().splitlines() # read entire blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Description of Processing__\n",
    "\n",
    "Use regular expressions in an attempt to match patterns of tokens in the input file. The regex will match the following items (in order of priority):\n",
    "* Various contractions ending in a period captured as one token (e.g. Dr., Mr., St.)\n",
    "* Twitter-like hashtags beginning with pound sign and alphanumeric characters\n",
    "* dollar amounts in the form of `$2.534` or `$5`\n",
    "* E-mail addresses in the form of alphanumerics@alphanumerics.alphanumerics\n",
    "* Acronym in the form of `A.B.C.D.E.` up to an arbitrary amount\n",
    "* Consecutive alphanumeric characters (e.g. `[a-zA-Z0-9_]`)\n",
    "* Alphanumeric characters surrounding period, comma, colon, apostrophe, slashes, and hyphen (e.g. `I'm`, `do-rag`, `will.i.am`, `AC/DC`, and `www.ab-inbev.com` each as as one token)\n",
    "* Consecutive punctuations including the following: `[].,;:\"'?()-_`, to match single and multi-character punctuations (e.g. ellipsis and em-dashes), as well as emoticons, such as `:(`.\n",
    "\n",
    "At the end of the tokenization, all tokens are converted to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r''' Dr\\. | Mr\\. | Mrs\\. | Ms\\. | St\\. | Mt\\. # matches various contractions\n",
    "    | \\#\\w+                                         # matches hashtags like on Twitter\n",
    "    | \\$\\d+(?:\\.\\d+)                                # matches dollar decimals\n",
    "    | \\w+\\@\\w+\\.\\w+                                 # matches e-mails\n",
    "    | \\w(?:\\.\\w)+\\.                                 # matches acronyms\n",
    "    | \\w+(?:[/\\.,’:\\'—-]\\w+)*%*                     # general match\n",
    "    | [*[\\]!<>=^{}|&.,;:?()_\\\"\\'\\-]+                # consecutive punctuations\n",
    "    '''\n",
    "tokenizer = re.compile(pat, flags=re.I|re.X)\n",
    "\n",
    "def tokenize(txt):\n",
    "    return [s.lower() for s in tokenizer.findall(txt)]\n",
    "\n",
    "################################################################################\n",
    "#tokenize.findall('adg 23:245 I\\'m groot, U.S.A. 2*3 HIV+ #abc, 12.4% 4534=4534 A&W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Errors and Undesirable Results__\n",
    "\n",
    "Initially, I separated the file with strings of whitespaces and also captured all punctuations separately. However, this lead to cases where words with punctuation within it (such as URL and hyphenated words) were split up as separate tokens. I added regex to capture for single punctuation being surrounded by alphanumeric strings as one token. The list of punctuation to be included in this scheme was discovered via trail and error. I initially started with just hypens and periods, but found special cases like colon (for timestamps like `12:45`), slashes, and apostrophes. Further complicating this is that sometimes single quotes are used as apostrophes, as em-dashes used for hyphens.\n",
    "\n",
    "The next thing I noticed was that e-mails would be captured as two tokens before and after the at sign, so a case for e-mail was added. With this, I also added special cases for Twitter-like hashtags as well as dollar amounts to the case of $2.53. Lastly, I also noticed that several contractions like \"Dr.\" would be tokenized as 'Dr' and '.', so I added a list of special cases for common contractions. Noticing that sometimes acronyms may have a period at the end, I also added a special case for them.\n",
    "\n",
    "Unfortunately, despite all the work, many undesirable results still may remain. For example, for tokens using symbols like \"HIV+\" or other currencies like \"¥400\", the tokenizer will still fail. Moreover, there are many possible contractions that I had not taken care of, such as \"Sgt.\", \"Fr.\", or \"Ct.\". It also cannot distinguish between the last period of an acronym vs. an actual period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Demonstration of Results__\n",
    "\n",
    "The first ten sentences (manually counted) are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Russian for plastic bag is полиэтиленовый пакет. 7.3 out of 10 statistics is made up. I do not like green eggs and ham.I do\\nnot like them Sam-I-Am. Dr. Mulholland lives on Mulholland Dr. in Hollywood. 1, 2, 3... slashdot.com has some interesting\\narticles. I'm going to update my resumé. J.H.U. has a great la-crosse team. Born in the U.S. of A. \"\n"
     ]
    }
   ],
   "source": [
    "print(repr(tokBlob[:345]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of tokenization are presented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['russian', 'for', 'plastic', 'bag', 'is', 'полиэтиленовый', 'пакет', '.', '7.3', 'out', 'of', '10', 'statistics', 'is', 'made', 'up', '.', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham.i', 'do', 'not', 'like', 'them', 'sam-i-am', '.', 'dr.', 'mulholland', 'lives', 'on', 'mulholland', 'dr.', 'in', 'hollywood', '.', '1', ',', '2', ',', '3', '...', 'slashdot.com', 'has', 'some', 'interesting', 'articles', '.', \"i'm\", 'going', 'to', 'update', 'my', 'resumé', '.', 'j.h.u.', 'has', 'a', 'great', 'la-crosse', 'team', '.', 'born', 'in', 'the', 'u.s.', 'of', 'a', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(tokBlob[:345]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Corpus Statistics\n",
    "\n",
    "Processing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = Counter(tokenize(tokBlob)) # counter is set with counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Basic Statistics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 944802\n",
      "Vocabulary size: 396931\n",
      "Collection size: 22233607\n"
     ]
    }
   ],
   "source": [
    "print('Number of lines: %d'%tokBlob.count('\\n'))\n",
    "print('Vocabulary size: %d'%len(vocabs))\n",
    "print('Collection size: %d'%sum(vocabs.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Most common types__\n",
    "\n",
    "List of most common types at rank 1-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: the    \t51: up\n",
      "2: .    \t52: when\n",
      "3: ,    \t53: her\n",
      "4: to    \t54: there\n",
      "5: and    \t55: can\n",
      "6: of    \t56: also\n",
      "7: a    \t57: out\n",
      "8: in    \t58: would\n",
      "9: for    \t59: people\n",
      "10: is    \t60: new\n",
      "11: that    \t61: if\n",
      "12: on    \t62: which\n",
      "13: with    \t63: so\n",
      "14: was    \t64: what\n",
      "15: at    \t65: time\n",
      "16: it    \t66: your\n",
      "17: as    \t67: after\n",
      "18: be    \t68: its\n",
      "19: are    \t69: my\n",
      "20: i    \t70: two\n",
      "21: he    \t71: )\n",
      "22: said    \t72: ?\n",
      "23: this    \t73: first\n",
      "24: have    \t74: some\n",
      "25: from    \t75: just\n",
      "26: by    \t76: do\n",
      "27: will    \t77: no\n",
      "28: we    \t78: year\n",
      "29: \"    \t79: other\n",
      "30: has    \t80: years\n",
      "31: you    \t81: than\n",
      "32: but    \t82: like\n",
      "33: not    \t83: them\n",
      "34: they    \t84: over\n",
      "35: an    \t85: into\n",
      "36: his    \t86: get\n",
      "37: :    \t87: ,\"\n",
      "38: their    \t88: now\n",
      "39: or    \t89: only\n",
      "40: who    \t90: last\n",
      "41: more    \t91: many\n",
      "42: all    \t92: school\n",
      "43: one    \t93: how\n",
      "44: (    \t94: us\n",
      "45: about    \t95: .\"\n",
      "46: were    \t96: state\n",
      "47: she    \t97: because\n",
      "48: had    \t98: could\n",
      "49: been    \t99: most\n",
      "50: our    \t100: these\n"
     ]
    }
   ],
   "source": [
    "tmp = vocabs.most_common(10000)\n",
    "for n in range(50):\n",
    "    print('%d: %s    \\t%d: %s'%(n+1,tmp[n][0],n+51,tmp[n+50][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common at 500, 1000, 5000, and 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: makes\n",
      "1000: trade\n",
      "5000: valid\n",
      "10000: .;\n"
     ]
    }
   ],
   "source": [
    "print('500: %s'%tmp[499][0])\n",
    "print('1000: %s'%tmp[999][0])\n",
    "print('5000: %s'%tmp[4999][0])\n",
    "print('10000: %s'%tmp[9999][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hapex legomena__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239422 words or 60.32% of vocab.\n"
     ]
    }
   ],
   "source": [
    "tmp = [s for s,c in vocabs.items() if c==1]\n",
    "print('%d words or %.2f%% of vocab.'%(len(tmp),len(tmp)/len(vocabs)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Zipf's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = vocabs.most_common(len(vocabs))\n",
    "plotY = np.log(np.array([c for s,c in tmp]))\n",
    "plotX = np.log(np.arange(len(vocabs))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZgcZb328e9NFhISloQMCCFhYMREBFkcZF+EoEhY1AOCr0qCAoejRwR5haCyqEcJL6KCqBAQAsjBw+ZRSBQBgYBgYILsECUwIWHLQAgJSWCy/N4/qiZ0Jt0zPUt3dU/fn+vqK91V1V2/GYa7nn7qqacUEZiZWe1YL+sCzMysvBz8ZmY1xsFvZlZjHPxmZjXGwW9mVmMc/GZmNcbBbyUh6U+SJhS57eaSZkhaIumiUtdWqSQ1SxqXdR3W9zn4rcskfVHSO3keIekcgIj4dERcU+RHngS8AWwUEadLOk/Sed2oKyR9sKvvK5VqC3JJ90o6oQvbV9XPZ+9z8FuXRcT1ETE09wGcCrwOXNGNj9waeCZ8NaFZWTj4rcck7QL8DDg2Il5Nl61pPUqaKOlvkn4h6W1Jz0k6KF03FZgAnJF+axjX7rNHSLpd0iJJCyXdL6lLf7eSGiT9VdKbkt6QdL2kTdJ1x0u6LWfb5yXdmPN6nqSdu/E7uQ4YDdyW/lxnpMuPkPR0+vPcK+nDBd4/VtKLko5NX28p6RZJLenyU3K2PU/SjZKuTbvLnpbUmLP+TEkvp+tmt/3u2+3vR8C+wKVpvZdK2iv9fY1Kt9kprXtsoZ/PqkRE+OFHtx/AJsAc4Mx2y+8FTkifTwRWAqcBA4BjgLeB4en6qcB/Ffj884HL0vcNIAknFdg2gA/mWf5B4GBgfaAOmAH8PF23LbCIpBG0BTAXeDln3VvAet383TQD43JefwhYmtYyADgDeB4YmLs9sCvwEnBYunw9YBZwDjAwresF4FPp+vOAd4FDgX7p7+zv6boxwDxgy/R1PdBQoN41/81ylv0I+CswGHgC+M9CP58f1fNwi9+6TZKAa4CngP/XyeYLSMJ2RUT8DzAbGF/EblaQBPLW6XvvjzR1ihURz0fEnRHxXkS0AD8F9k/XvQAsAXZOl90BvCxpbPr6/ohY3ZX9deAYYFpaywrgJySBulfONvsCfwQmRMTt6bLdgLqI+EFEtKY1XwEcm/O+ByJiekSsAq4DdkqXryI54G0vaUBENEfEnC7UfB6wMfAw8Arwyy681yqUg9964kxgB5KQ6iyMX263zVxgyyL2cSFJq/gvkl6QNKmrRUraTNLv0u6OxcBvgRE5m9wHHADslz6/lyT0909f5/vMy3JOan+nyFK2JPm5AUgPKPOAkTnbnAw8GBH35CzbGtgy7WZZJGkR8B1g85xtXst5vgwYJKl/RDxPcv7lPGBB+nso5vfeVuMKkm9kOwAXdfWga5XJwW/dIukA4LvAURGxqIi3jEy/IbQZTdKC7FBELImI0yNiW+Bw4Fv5+qg7cT5JN9BHI2Ij4EtAbi1twb9v+vw+Ogn+iDg53j+5/eNC5bd7/QpJiANrvjGNAl7O2eZkYLSkn+Usmwe8GBGb5Dw2jIhDO/qhc2r974jYJ913ABcUWS+SRgLnAlcDF0lav6PtrTo4+K3LJG0B/A44NSL+UeTbNgNOkTRA0tHAh4HpRezrMEkfTENyMUnXxaoO3jJQ0qCcRz9gQ+AdYFEaZN9u9577gE8AgyNiPnA/cAiwKVDsz5fP6yT98W1uBMZLOkjSAOB04D3gwZxtlqT73k/S5HTZw8Di9CTtYEn9JO0gabfOCpA0RtKBaWC/Cyyn8O9vrXrT3/lU4DfAV4FXgR928PNZlXDwW3ecSNLNcLHWHct/WYH3zAS2Ixmv/yOSbwpvFrGv7YC7SIL7IeBXEXFvB9s/TRJubY/jge+TnDB9G5gG3Jr7hoj4Z/r596evF5OcPP1b2mfeXecD30u7Z/5vRMwm+bbxC5Lfw+HA4RHR2q6eRSQngD8t6YdpDYeTnId4MX3vlSR9751ZH5icvuc1kgPwd2DN9RhP52x7MXCUpLckXQKcQvLf+ey0i+d44HhJ++b7+br0m7FMyV12VmqSJpKMFtkn61rMzC1+M7Oa4+A3M6sx7uoxM6sxbvGbmdWY/lkXUIwRI0ZEfX191mWYmVWVWbNmvRERde2XV0Xw19fX09TUlHUZZmZVRdLcfMvd1WNmVmMc/GZmNcbBb2ZWYxz8ZmY1xsFvZlZjShb8kq6StEDSU3nW/V8lN8Yeke+9ZmZWOqVs8U8lmV52Len9Ow8mubWcmZmVWcmCPyJmAAvzrPoZyb1GSz5XxMKlrVx+3xwWLm3tfGMzsxpR1j5+SUeQ3ILv8SK2PUlSk6SmlpaWbu3vpqZ5nP+n57ipaV633m9m1heV7cpdSRuQ3Krvk8VsHxFTgCkAjY2N3fp2cHTjqLX+NTOz8rb4G4BtgMclNQNbAY9K+kCpdjh8yED+ff8Ghg8Z2O3PKKa7yF1KZlZNyhb8EfFkRGwWEfURUQ/MB3aNiNfKVUN3FNNd5C4lM6smJevqkXQDcAAwQtJ84NyI+E2p9lcqxXQXuUvJzKpJVdyIpbGxMTw7p5lZ10iaFRGN7Zf7yl0zsxrj4DczqzEOfjOzGuPgLzEP9TSzSuPgL7F8Qz19MDCzLFXFPXerWb6hnm0HA4B/37+BhUtbualpHkc3jurRxWZmZsVw8JdY29XDudofDHwgMLNycvBnoP3BoLMDgZlZb3LwV4DODgRmZr3JJ3crUO7kcj4RbGa9zcFf4TwBnJn1Nnf1VDh3+5hZb3PwV7h8o4LMzHrCXT1mZjXGwW9mVmMc/GZmNcbBb2ZWYxz8VcRj+s2sNzj4q0jumH4fBMysu0p5s/WrgMOABRGxQ7rsQuBwoBWYAxwfEYtKVUNfkzumv+0gsKx1JRsM7O8J3cysaKVs8U8FDmm37E5gh4j4KPBP4KwS7r/PyZ3K4ejGUZz16bGAfGWvmXVJyVr8ETFDUn27ZX/Jefl34KhS7b+vazsILFzaygYD+zFu+825/L45bvmbWaeyvHL3K8D/FFop6STgJIDRo0eXq6aq03YAuPy+Oe76MbOiZBL8kr4LrASuL7RNREwBpgA0NjZGmUqrWm39/8taV3kufzPrUNmDX9IEkpO+B0WEA72X5Hb9QLCsdRVzWt7hrmded+vfzNZS1uCXdAhwJrB/RCwr575rxfAhA9lgYH/O/9NzPDF/EffMbnH3j5mtpZTDOW8ADgBGSJoPnEsyimd94E5JAH+PiJNLVUOtauv2Gbf95uyx7etrun/+/sKbXPT5nR3+ZjVO1dDb0tjYGE1NTVmXUbUWLm3l9Bsf457ZLXxiTJ3D36xGSJoVEY3tl3s+/howfMhALvr8zmvC/5oHXwRgeetqBg/sx4S96n0gMKshDv4a0Rb+NzXNY1nrKi6++19r1jU1L+QjW24M4AOBWQ1w8NeQ9iN/lreu5plXF/PA82/wtzlvrtnuifmL3B1k1oc5+GvQ8CEDOe3gMUDS/3/Ngy+yvHU1AM+8uph7Zrfwjf9+lMb64W79m/VBPrlra8k9EQzQUDeEKcc10lA3NOPKzKyrCp3cdfDbOtq+BfzhsVdofnMZ9ZtuwH7b1TH79SWM+cCGDNtgoL8JmFUBj+qxorV1BR2x80hOuraJOS1LaX5zLgAzX1yYbhVruovMrLo4+K2ghrqh3HTyXlzz4Iu8tXQFs19fwopVq3n0pUXMmruIhUtb3eo3q0IOfutQ7olgWPscwOk3PubRP2ZVyLdetC5pux5g74ZN17oYzMyqh4Pfumz4kIE01g8H4NZHX+bH05/1vX/NqoiD37plwl71NNQNYd5by5ky4wWOvPQBjrn8Iea0vJN1aWbWCQe/dcvwIQOZclwjezdsyqhhg5n31nJmvriQoy970OFvVuE8jt96bE7LO5x1yxPMfn0Jby9fyfr9khFBgwb2p7F+OCenN4g3s/LyBVxWcnNa3uGwS+5n+YrVay0fNWwwU7/ycV/9a1ZmhYLfXT3WaxrqhnL9CXswathgtv/AUDYalIwWnvfWcg675H4enftWxhWaGbjFbyXU1gX0xPy3eXflagYPWI/bT9nXLX+zMnFXj2Xm0blv8X+ueIh3VwYDBJtuuD6f3P4DnHrwh9z3b1ZC7uqxzOy69TCO22sbAFYEvLb4Pa79+1z2ueAu9vjxXZzzv0/5OgCzMipZ8Eu6StICSU/lLBsu6U5J/0r/HVaq/VtlOXn/Bo7bYzSbb7g+6/cTAMtaY81B4LJ7n8+4QrPaUcoW/1TgkHbLJgF3R8R2wN3pa6sBw4cM5Aef2ZGZ3x3H9FP3Y5dRG1M3ZCAD07/Ax+e/nW2BZjWkZMEfETOAhe0WHwlckz6/BvhMqfZvlauhbii///o+PHL2wUzcZ1sAHp+3iHufW5BxZWa1odx9/JtHxKsA6b+bFdpQ0kmSmiQ1tbS0lK1AK6+T929g8ID1eHflaiZOfYQjLn3AV/6alVjFntyNiCkR0RgRjXV1dVmXYyUyfMhAfv3Fj6H09RPz32b8xR7zb1ZK5Q7+1yVtAZD+6+/2xgFjN+PqibvRL/1rfHflaj736wfZ8bw73P1jVgLlDv4/AhPS5xOAP5R5/1ahDhi7GX85bX+23HjQmmVL3l3JxKmPsM2kaXzk3D/7IGDWS0o5nPMG4CFgjKT5kr4KTAYOlvQv4OD0tRmQnPS9/ZR9+fzHRjKg3/vLA1j63iomTn2EG2bOzaw+s77CV+5aRVq4tJXv//FJpj3+Givbrdtsw/W57EsfY9etfRmIWUc8ZYNVrYVLW/nOrY/z56fX7ur53qFjOWG/hoyqMqt8Dn6rerc99jLf+N1j6yw/aGwdFx7tm76btee5eqzqHb7zSG79j70YNrj/Wsvvfq6FfS+420NAzYrk4LeqsuvWw/jHuZ/i/M/usNbypa3JEFCf/DXrXP/ONpA0CDgM2BfYElgOPAVMi4inS1ueWX5f2H1rvrD71twwcy5n/X7NPIBrnn9h962zKs2s4nXY4pd0HvA3YE9gJnA5cCOwEpiczrD50VIXaVbIF3bfmqkTd2M9vb/srN8/xScuvMdTP5gV0OHJXUnjI2JaB+s3A0ZHREnPvPrkrnVmTss7HH7J/Sxrd7/f9QQ/+swO/gZgNalbJ3fbQl/SDgXWLyh16JsVo6FuKLedsi9bDxu01vLVkXwDGPu9P/nKX7NUsSd3L5P0sKSvSdqkpBWZdVND3VDuO/Mgvnfo2HXWtc3+ueN5f/boH6t5RY/jl7Qd8BXgaOBh4OqIuLOEta3hrh7rjnufW8CJ1zzCijx/4gJ+/Fl3AVnf1isXcEnqR3LzlEuAxST//3wnIm7trULzcfBbT7Qf+dPet8Z9kFPGjSljRWbl0aPgT0fuHA+MB+4EfhMRj0raEngoIkrabHLwW2+45K7Z/PSuwvf2PfQjm/GrL+9WxorMSqunwT8DuAK4OSKWt1v35Yi4rtcqzcPBb72ps28A48aM4Mrjdy9jRWal0dPgHwosj4hV6ev1gEERsazXK83DwW+lUGjunzb7NQzn2hP3LGNFZr2rp3P13AUMznm9QbrMrGodvvNImieP5+7T92fj9bXO+hlzFlI/aRqX3DU7g+rMSqfY4B8UEWsug0yfb1CakszKq6FuKI9//1DuPn1/NswzwedP73qe+knTuHLGnPIXZ1YCxQb/Ukm7tr2Q9DGSOXvM+oyGuqE8+YPx3Pofe9Evz/r/mv4c20yaxm2PvVz22sx6U7F9/LsBvwNeSRdtARwTEbNKWNsa7uO3LJz3hyeZ+tBLedetJ7hqwm4cMHazMldlVrwej+OXNAAYQzJ2/7mIWNG7JRbm4Lcsfe26R5j+dP7pHgRcPdEHAKtMvRH8ewH15EzlHBHXdrOY04ATSO6j/SRwfES8W2h7B79VgqN+eT9N8xbnXVc/bBD3nnlQmSsy61hPh3NeBzQAjwGr0sUREad0o5CRwAPA9hGxXNKNwPSImFroPQ5+qySfuugeZrfkH8lct0F/HjnnU2WuyCy/QsHf6Y1YUo0kQd1bN+jtDwyWtIJkdNArnWxvVjHuOP0TAOx7/l3Me/u9tda1LFtJ/aRpjNp4fe4/a1wW5Zl1qthRPU8BH+iNHUbEy8BPgJeAV4G3I+IvvfHZZuV0/1nj+MWxO+ddN+/t96ifNI1PXXRPmasy61yxXT33ADuTzMq5pokTEUd0eYfSMOAW4BhgEXATyVQQv2233UnASQCjR4/+2Ny5vpeqVa5H577F5379YMH1mw8ZwMyzP1nGisx63se/f77lEXFfNwo5GjgkIr6avj4O2CMivlboPe7jt2pxwfRn+PWMFwuu33LDgTz43YPLWJHVsh5N2ZAGfDMwIH3+CPBoN2t5CdhD0gaSBBwEPNvNzzKrKGceuj3Nk8ezX8PwvOtfWdJK/aRpHHfFQ2WuzOx9RQW/pBOBm0lutg4wEvjf7uwwImamn/UoyVDO9YAp3fkss0p17Yl70jx5POPGjMi7vm0eoIVLW8tcmVnxXT2PAR8HZkbELumyJyNixxLXB7irx6rfkZfcx+OvvJN3XcOmg7n72weWuSKrBT2dnfO9iFjTNJHUn+TiKzMrwh9O2Z/myePzTgI3583l1E+aVv6irGYVG/z3SfoOydj7g0lG4txWurLM+qYnfzCe5snj866rnzSNHc/xAcBKr9jgnwS0kPTJ/zswHfheqYoy6+sKtf6XtCYHgAumP1P+oqxmdOlm61lxH7/1ZR118xT6dmBWjB718Ut6UdIL7R+9X6ZZ7WmePD7vHcAgOSh87bpHylyR9XXFjurZNOflIOBoYHhEnFOqwnK5xW+1wq1/6009vYDrzZzHyxHxc8Djz8x6WfPk8exZv0nedfWTprHLeX8qc0XWFxXb1bNrzqNR0snAhiWuzawm3XDy3gVb92+9u9pDP63Hih3Vc1HO43zgY8DnS1WUmSWt/4l7js67zn3/1hMe1WNWBdz3b93R09k5v9XR+oj4aQ9q65SD3wzG/+xenn59ad51jaM24uav71vmiqzS9XTKhkbgP0gmZxsJnAxsT9LP775+szKYdtoBBVv3TfMWu+/filZs8I8Ado2I0yPidJI+/q0i4vsR8f3SlWdm7XU066fD34pRbPCPBnLnj20F6nu9GjMrypXH797hnD8nXD2zzBVZNSk2+K8DHpZ0nqRzgZnAtaUry8yKUSj875r9hlv/VlCxF3D9CDgeeIvkPrnHR8SPS1mYmRWnefJ4GjYdnHedw9/yKbbFD7ABsDgiLgbmS9qmRDWZWRfd/e0DO+z6+cjZPgDY+4q9cvdc4EzgrHTRAOC3pSrKzLqnUPgvXeHWv72v2Bb/Z4EjgKUAEfEKHsZpVpGaJ4/n2MaRedc5/A2KD/7WSK70CgBJQ0pXkpn11OSjdu6w62e3H9xR5oqskhQb/DdKuhzYRNKJwF3AFd3dqaRNJN0s6TlJz0ras7ufZWaFFQr/lmUr3fqvYcWO6vkJcDNwCzAGOCciftGD/V4M/DkixgI7Ac/24LPMrAPNk8ez05ZD865z+NemTufqkdQPuCMixvXKDqWNgMeBbaPIGeI8V49Z7ygU9J7orW/q9lw9EbEKWCZp416qZVuSG7dfLekfkq7Md85A0kmSmiQ1tbS09NKuzWpbR/3+e/3ozjJXY1kpto//XeBJSb+RdEnbo5v77A/sCvw6InYhGSk0qf1GETElIhojorGurq6buzKz9gqF/ytLWt31UyOKDf5pwNnADGBWzqM75gPzI6JtMpGbSQ4EZlYmzZPH871Dx+Zd5/Dv+zoMfkl3p0+3j4hr2j+6s8OIeA2YJ2lMuugg4JnufJaZdd8J+zV02PVjfVdnLf4tJO0PHCFpl3b33u1JK/0bwPWSngB2Bjzvj1lGHP61p8NRPZKOAr4K7AO0H1YTEXFgCWtbw6N6zEqvUNDv1zCca0/0pTbVqKe3Xjw7In5YksqK4OA3Kw/f27dv6dZwTkn1AIVCX4mteqNAM8te8+TxbDq4X9517vrpOzrr479Q0i2SjpP0EUmbSRot6UBJPwT+Bny4DHWaWZnMOvcQ9/v3cR0Gf0QcTTKMcwzwS+B+4I/AicBs4MCI8FUfZn2Qw7/vKqqPP2vu4zfLjqd5qF7dnrIhffPn8jwOkrRZ75dqZpXELf++p9grd78KXAl8MX1cAXwL+JukL5eoNjOrEA7/vqXY4F8NfDgi/i0i/g3YHngP2J3kloxm1sd1FP47nuMDQDUpNvjrI+L1nNcLgA9FxEJgRe+XZWaVqFD4L2l167+aFBv890u6XdIESRNIRvbMSKdTXlS68sys0jRPHs/EPUfnXefwrw7FXrkr4HMkUzcIeAC4pdgbqfSUR/WYVSaP+KlsPRrVkwb8A8BfSe63O6NcoW9mlcsnfatTscM5Pw88DBwFfB6YmU7gZmY1zuFffYrt4/8usFtETIiI44CPk1zRa2bm8K8yxQb/ehGxIOf1m114r5nVAId/9Sg2vP8s6Q5JEyVNJLkV4/TSlWVm1cjhXx2KPbn7bWAK8FFgJ2BKRPjCLTNbh8O/8hXdXRMRt0TEtyLitIj4fSmLMrPq5vCvbJ3diGWJpMV5HkskLS5XkWZWfRz+lauz+fg3jIiN8jw2jIiNerJjSf0k/UPS7T35HDOrXA7/ypTlyJxvAs9muH8zKwOHf+XJJPjT+/SOJ5nq2cz6OId/Zcmqxf9z4AyS6Z7zknSSpCZJTS0tLeWrzMxKwuFfOcoe/JIOAxZExKyOtouIKRHRGBGNdXV1ZarOzErJ4V8Zsmjx7w0cIakZ+B1woKTfZlCHmWXA4Z+9sgd/RJwVEVtFRD1wLPDXiPhSuesws+w4/LPl+XbMLBMO/+xkGvwRcW9EHJZlDWaWHYd/NtziN7NMOfzLz8FvZplrnjyeIQPWXe7wLw0Hv5lVhKd/6JZ/uTj4zaxiuNunPBz8ZlZRHP6l5+A3s4rj8C8tB7+ZVSSHf+k4+M2sYjn8S8PBb2YVrXnyeIYNWjeqHP7d5+A3s4r3j/M+nXe5w797HPxmVhXc8u89Dn4zqxpu+fcOB7+ZVZVCJ3yPvOS+MldSvRz8ZlZ18oX/46+8k0El1cnBb2ZVKV/410+a5m6fIjj4zaxqeZx/9zj4zayqNY7aKO9yh39hDn4zq2o3f31ft/y7yMFvZn1CofC3dZU9+CWNknSPpGclPS3pm+Wuwcz6Jp/wLU4WLf6VwOkR8WFgD+DrkrbPoA4z64M66vbxASBR9uCPiFcj4tH0+RLgWWBkueswM6tVmfbxS6oHdgFmZlmHmfUtzZPH+4RvBzILfklDgVuAUyNicZ71J0lqktTU0tJS/gLNrOp1dACoZZkEv6QBJKF/fUTcmm+biJgSEY0R0VhXV1feAs3M+jBFRHl3KAm4BlgYEacW857GxsZoamoqbWFm1qcdcMHdNL/17jrL+/I3AkmzIqKx/fIsWvx7A18GDpT0WPo4NIM6zKyG5Av9WtW/3DuMiAcAlXu/Zlbb6ocNyhv+uSd7+3LrP1fZg9/MLAv3nnnQmue1PrLHUzaYmdWYsp/c7Q6f3DWzUinU+u8L3T6VdHLXzMwy5OA3M6sxPrlrZjWtL3TpdJWD38ws1dlon75ykHBXj5lZjXHwm5nVGHf1mJml+kpXTmfc4jczqzEOfjOzGuOuHjOzDnR1Xp9q6C5yi9/MrMY4+M3Maoy7eszMOlANXTdd5Ra/mVmNcfCbmdUYB7+ZWY1x8JuZ1ZhMgl/SIZJmS3pe0qQsajAzq1VlD35J/YBfAp8Gtge+IGn7ctdhZlarshjO+XHg+Yh4AUDS74AjgWcyqMXMrMe6enVvV/X2kNIsunpGAvNyXs9Pl61F0kmSmiQ1tbS0lK04M7O+LovgV55lsc6CiCkR0RgRjXV1dWUoy8ysNmTR1TMfGJXzeivglQzqMDPrFdV2dW8WLf5HgO0kbSNpIHAs8McM6jAzq0llb/FHxEpJ/wncAfQDroqIp8tdh5lZrcpkkraImA5Mz2LfZma1zlfumpnVGAe/mVmNcfCbmdUYB7+ZWY1RxDrXTlUcSS3A3G6+fQTwRi+WU07VXDtUd/2uPRuuvXdtHRHrXAFbFcHfE5KaIqIx6zq6o5prh+qu37Vnw7WXh7t6zMxqjIPfzKzG1ELwT8m6gB6o5tqhuut37dlw7WXQ5/v4zcxsbbXQ4jczsxwOfjOzGtOng79ab+ouaZSkeyQ9K+lpSd/MuqauktRP0j8k3Z51LV0haRNJN0t6Lv3975l1TV0h6bT0b+YpSTdIGpR1TYVIukrSAklP5SwbLulOSf9K/x2WZY2FFKj9wvTv5glJv5e0SZY1dqTPBn+V39R9JXB6RHwY2AP4ehXV3uabwLNZF9ENFwN/joixwE5U0c8gaSRwCtAYETuQTHt+bLZVdWgqcEi7ZZOAuyNiO+Du9HUlmsq6td8J7BARHwX+CZxV7qKK1WeDn5ybukdEK9B2U/eKFxGvRsSj6fMlJOGzzn2JK5WkrYDxwJVZ19IVkjYC9gN+AxARrRGxKNuquqw/MFhSf2ADKvjudhExA1jYbvGRwDXp82uAz5S1qCLlqz0i/hIRK9OXfye5u2BF6svBX9RN3SudpHpgF2BmtpV0yc+BM4DVWRfSRdsCLcDVaTfVlZKGZF1UsSLiZeAnwEvAq8DbEfGXbKvqss0j4lVIGkDAZhnX011fAf6UdRGF9OXgL+qm7pVM0lDgFuDUiFicdT3FkHQYsCAiZmVdSzf0B3YFfh0RuwBLqdyuhnWk/eFHAtsAWwJDJH0p26pqj6TvknTXXp91LYX05eCv6pu6SxpAEvrXR8StWdfTBXsDR0hqJuleO1DSb7MtqWjzgfkR0fbt6maSA0G1GAe8GBEtEbECuBXYK+Oauup1SVsApP8uyLieLpE0ATgM+GJU8EVSfTn4q/am7pJE0s/8bET8NOt6uiIizoqIrSKinuR3/teIqIpWZ0S8BsyTNCZddBDwTIYlddVLwB6SNkj/hg6iik5Op/4ITEifTwD+kGEtXSLpEOBM4IiIWJZ1PR3ps9EZicoAAALqSURBVMGfnmRpu6n7s8CNVXRT972BL5O0lh9LH4dmXVSN+AZwvaQngJ2BH2dcT9HSbyo3A48CT5L8/12x0whIugF4CBgjab6krwKTgYMl/Qs4OH1dcQrUfimwIXBn+v/sZZkW2QFP2WBmVmP6bIvfzMzyc/CbmdUYB7+ZWY1x8JuZ1RgHv5lZjXHwW82Q9E4P33+zpG17qZaJki7Ns/w/JR3fG/swK8TBb1YESR8B+kXEC3nW9evFXV1FMsOmWck4+K3mKHFhOmf9k5KOSZevJ+lX6Xz2t0uaLumo9G1fJOcqUknvSPqBpJnAnpLOkfRI+plT0itnkXSvpAskPSzpn5L2zVPPeEkPSRqRXvHZLOnjpf9NWK1y8Fst+hzJVbk7kcxvc2E6L8zngHpgR+AEIPcmLHsDuRPPDQGeiojdI+IB4NKI2C2dB38wyXwtbfpHxMeBU4FzcwuR9FmSieAOjYg30sVNwDoHCLPe0j/rAswysA9wQ0SsIpkU7D5gt3T5TRGxGnhN0j0579mCZMrmNqtIJtFr8wlJZ5DMgT8ceBq4LV3XNsneLJIDy5r3AI3AJ9vNvroAGNv9H8+sY27xWy3KN2V3R8sBlgO5tzF8Nz1wkN7e8FfAURGxI3BFu23fS/9dxdqNrRdI5nb5ULt9DUr3Z1YSDn6rRTOAY9L7AteR3HXrYeAB4N/Svv7NgQNy3vMs8MECn9cW8m+k91A4qsB27c0l6V66Nj153OZDwFP532LWcw5+q0W/B54AHgf+CpyRTsl8C8mc/E8Bl5Pc9ezt9D3TWPtAsEZ6e8YrSGbE/F+SKcGLEhGzSU4c3ySpIV28N3BXl34isy7w7JxmOSQNjYh3JG1K8i1g74h4TdJg4J709aoS7n8X4FsR8eVS7cPMJ3fN1na7pE2AgcAP028CRMRySeeS3Lf5pRLufwRwdgk/38wtfjOzWuM+fjOzGuPgNzOrMQ5+M7Ma4+A3M6sxDn4zsxrz/wEIy7blWUrDSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(plotX, plotY, s=1)\n",
    "plt.title(\"Zipf's Law - tokens.txt\")\n",
    "plt.xlabel('log(rank)')\n",
    "plt.ylabel('log(frequency)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on visual examination, the corpus as tokenized by my tokenizer somewhat follows Zipf's Law, as the plot is generally inversely proportional between log of frequency and log of rank. However, the anonmalies at the very high and the very low ranks are apparent, and the fact that there seem to be an inflection point at the middle of the distribution. With these, it is unlikely that the data would pass something like a chi-square test for Zipf's distribution. It seems that this empirical distribution is piece-wise linear with a spline at around rank 1100 ($~e^7$) or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sentence Boundary Detection\n",
    "\n",
    "The sentence ending is detected via regular expression. The regular expression looks for the following pattern:\n",
    "1. alphanumeric character or close quotation mark followed by any number of consecutive sentence terminator (`!?.`)\n",
    "1. any number of white spaces followed by an optional close quotation\n",
    "1. a capital letter preceded by an optional open quotation mark\n",
    "\n",
    "If such pattern is found, it is deemed a sentence ending. Alternatively, the end of a line is also deemed as a sentence ending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentEnd = re.compile('([\\w+\\\"\\'”’][!?.]+[”\\\"]?)(\\s+)([“\\\"]?[A-Z])|$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceEnds(txt):\n",
    "    endings = []\n",
    "    for m in sentEnd.finditer(txt):\n",
    "        if m.end(2) == -1: # matches end of line\n",
    "            endings.append(m.start(0)-1) # append end of line\n",
    "        else: # matches period and start of new sentence\n",
    "            endings.append(m.start(2)-1) # append start of 2nd token)\n",
    "    return ' '.join(str(x) for x in [len(endings)] + endings)\n",
    "\n",
    "with open('./jwu74.txt', 'w') as f:\n",
    "    for s in sentLines:\n",
    "        f.write(getSentenceEnds(s)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the effectiveness, I checked the results of my program on selected lines. It seems that the results are correct for  all the lines I reviewed. However, I know for a fact that it would produce false positives for abbreviations like Dr. and Mr., as they are followed by white space with a capital letter. I would estimate the accuracy to be in the high 90's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) NLTK\n",
    "\n",
    "__Comparison of Tokenization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Russian', 'for', 'plastic', 'bag', 'is', 'полиэтиленовый', 'пакет', '.', '7.3', 'out', 'of', '10', 'statistics', 'is', 'made', 'up', '.', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham.I', 'do', 'not', 'like', 'them', 'Sam-I-Am', '.', 'Dr.', 'Mulholland', 'lives', 'on', 'Mulholland', 'Dr.', 'in', 'Hollywood', '.', '1', ',', '2', ',', '3', '...', 'slashdot.com', 'has', 'some', 'interesting', 'articles', '.', 'I', \"'m\", 'going', 'to', 'update', 'my', 'resumé', '.', 'J.H.U', '.', 'has', 'a', 'great', 'la-crosse', 'team', '.', 'Born', 'in', 'the', 'U.S.', 'of', 'A', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.word_tokenize(tokBlob[:345]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'march', 'to', 'see', 'Dr.', 'March', 'because', 'my', 'blood', \"'s\", 'B-', '#', 'transfusion']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(\"I march to see Dr. March because my blood's B- #transfusion\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization of NLTK is largely similar to mine. The most singificant difference is that it does not fold the cases of the token, and it breaks up contractions (e.g. `I'm` -> `I` `'m` and `It's` -> `It` `'s`). Furthermore, it does not attempt to capture acronyms with periods at the end. NLTK also tokenizes consecutive punctuation marks into separate tokens, except for cases of period, where they are tokenized together. NLTK also captures common abbreviations and tokenize the period at the end of the abbreviation together with the word (e.g. `Dr.`). It also seem to tokenize more corner cases correctly, for example things like `B-`, `HIV+`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comparison of Sentence Segmentation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There was a bigger discrepancy this year but nevertheless there was always a reduction in the results of the English students,\" Tremblay said.', 'GALLOWAY: You did these amazing films in the 70s, just extraordinary films, and what’s been great for me is getting to see them again.']\n",
      "[('d.', ' ', 'G'), ('', '', '')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.sent_tokenize(sentLines[20]))\n",
    "print(sentEnd.findall(sentLines[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Anybody who thinks we\\'re going to go to Pluto and find cold, dead rock is in for a rude awakening,\" said Bill McKinnon, a co-investigator for the New Horizons mission.', 'But the continued glut is starting to discourage that strategy.', \"Community experts are involved in the process, too, making sure programs are lined up to meet the newcomers' needs.\"]\n",
      "[('n.', ' ', 'B'), ('y.', ' ', 'C'), ('', '', '')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.sent_tokenize(sentLines[1920]))\n",
    "print(sentEnd.findall(sentLines[1920]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The best is Dr. Smith, who has 30 years of experience?!',\n",
       " 'He is also very kind... Not to mention hard-working']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"The best is Dr. Smith, who has 30 years of experience?! He is also very kind... Not to mention hard-working\"\n",
    "nltk.sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r.', ' ', 'S'), ('e?!', ' ', 'H'), ('d...', ' ', 'N'), ('', '', '')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentEnd.findall(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence segmenter from NLTK seems to be producing largely the same result out of spot check of a few sentences. The only major difference is that NLTK catches more exceptions like \"Dr.\" and does not tokenize ellipses as the end of a sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_jwu)",
   "language": "python",
   "name": "conda_jwu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
