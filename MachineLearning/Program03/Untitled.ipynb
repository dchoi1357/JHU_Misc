{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwiseDist(x, y=None):\n",
    "    if y is None:\n",
    "        y = x\n",
    "    return np.sum((x[:,None]-y)**2,axis=2)**0.5\n",
    "\n",
    "def prepData(dataPathDir, fieldNames, featSlices, sep=',', transf=None):\n",
    "    raw = pd.read_csv(dataPathDir , sep=sep, names=fieldNames) # read dlm file\n",
    "    dataFeats = fieldNames[featSlices] # list of feature names\n",
    "    if transf is None: # no transformation\n",
    "        dataMat = raw[dataFeats].values # original values\n",
    "    elif transf.lower() == 'std' : # if choose to standardize data\n",
    "        meanVals = raw[dataFeats].mean().values # mean of all features\n",
    "        stdVals = raw[dataFeats].std().values # standard deviations\n",
    "        dataMat = (raw[dataFeats].values - meanVals) / stdVals # [x-E(x)]/S(X)\n",
    "    elif transf.lower() == 'rescale': # rescale to values in [0,1]\n",
    "        mins = raw[dataFeats].min().values # min of feature vals\n",
    "        maxs = raw[dataFeats].max().values # max of feature vals\n",
    "        dataMat = (raw[dataFeats].values-mins) / (maxs-mins) # x-min/range(x)\n",
    "    else: # error out\n",
    "        raise Exception('No such transformation available')\n",
    "    return dataMat,dataFeats,raw['class'].values\n",
    "\n",
    "def errRate(pred, actual, categorical=True):\n",
    "    if categorical: # if categ., return classification err rate\n",
    "        return sum(pred!=actual) / pred.size\n",
    "    else: # if numeric, return RMSE\n",
    "        return np.linalg.norm(pred-actual)/np.sqrt(pred.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCommonElem(elems):\n",
    "    counts = dict() # dict to keep track of counts\n",
    "    for e in elems: # loop over array\n",
    "        counts[e] = counts.get(e,0) + 1 # increase count by 1 (def count of 0)\n",
    "    maxCount = -1\n",
    "    for e,ct in counts.items(): # loop over counts, set maxCount if count larger\n",
    "        maxCount = ct if ct > maxCount else maxCount\n",
    "    # get list of elems which has same count as maxCount (if multiple elems)\n",
    "    mostFreqElem = [e for e,ct in counts.items() if ct==maxCount]\n",
    "    return np.random.choice(mostFreqElem) # randomly choose one elemn from all\n",
    "\n",
    "def kMinValIdx(mat, k):\n",
    "    mat = np.copy(mat) # create copy of variable\n",
    "    if mat.ndim == 1:\n",
    "        mat = mat.reshape([1,-1])\n",
    "    idx = np.ones(mat.shape,int).cumsum(axis=1)-1 # rows of idx: 0,1,...,nCol\n",
    "    \n",
    "    for it in range(k): # perform k bubbles to get k smallest\n",
    "        for col in range(mat.shape[1]-it-1):\n",
    "            toSwap = mat[:,col] < mat[:,col+1] # if elem smaller than next elem\n",
    "            # swap cols of data matrix and matrix of indices\n",
    "            mat[np.ix_(toSwap,[col,col+1])] = mat[np.ix_(toSwap,[col+1,col])]\n",
    "            idx[np.ix_(toSwap,[col,col+1])] = idx[np.ix_(toSwap,[col+1,col])]\n",
    "    return idx[:,-k:],mat[:,-k:] # return smallest elemenst per row and the idxs\n",
    "\n",
    "################################################################################\n",
    "def KNN(trainX, trainY, testX, K, regression=False):\n",
    "    dists = pairwiseDist(testX,trainX) # all pairwise dist of two datasets\n",
    "    knnIdx,_ = kMinValIdx(dists, K) # idx of K closest data pts in training set\n",
    "    knnLabels = trainY[knnIdx] # labels of these closest data points\n",
    "    \n",
    "    testY = np.empty(testX.shape, trainY.dtype) # pre-allocate test data labels\n",
    "    if regression: # regression, calculate mean\n",
    "        testY = knnLabl.mean(axis=1) # mean of k-closest label values\n",
    "    else: # classification, get most common class label\n",
    "        testY = np.array([mostCommonElem(lab) for lab in knnLabels])\n",
    "    return testY # return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickAndRemove(arr):\n",
    "    j = np.random.randint(arr.size)\n",
    "    return arr[j], np.delete(arr,j)\n",
    "    \n",
    "\n",
    "def connsistentSubset(trainX, trainY, K=1):\n",
    "    dists = pairwiseDist(trainX) # all pairwise dist of two datasets\n",
    "    idx = np.arange(dataMat.shape[0]) # construct index of data rows\n",
    "    Z,idx = pickAndRemove(idx) # randomly pick 1st pt of of subset \n",
    "    \n",
    "    converged = False\n",
    "    while not converged:\n",
    "        converged = True # stop unless a misclassification\n",
    "        np.random.shuffle(idx) # shuffle sequence of sample to train randomly\n",
    "        for x in idx: # loop over all samples\n",
    "            nnIdx = kMinValIdx(dist[np.ix_(x,Z)], 1)[0] # idx of NN in Z\n",
    "            nnLabel = trainY[nnIdx].flatten() # label of NN of x in Z\n",
    "            if nnLabel!=trainY[x]: # if misclassification\n",
    "                Z = np.hstack(Z,x) # add to consistent subset\n",
    "                converged = False # continue training\n",
    "        idx = np.setdiff1d(idx, Z) # remove training set from samples\n",
    "        \n",
    "    return Z, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [2, 6],\n",
       "       [5, 6],\n",
       "       [4, 6],\n",
       "       [3, 8]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10)[kMinValIdx(j, 2)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def getCrossValidFolds(dataMat, classVec, nFolds=5, categorical=False):\n",
    "    ''' Cut N-fold cross validation of the data set\n",
    "    '''\n",
    "    \n",
    "    idx = np.arange(dataMat.shape[0]) # construct index of data rows\n",
    "    if categorical:\n",
    "        unqs = np.unique(classVec)\n",
    "        tmpHold = [None] * len(unqs)\n",
    "        for n,k in enumerate(unqs):\n",
    "            grpIdx = idx[classVec==k] # idx of all elems in current class\n",
    "            np.random.shuffle(grpIdx) # permutate idx for random selection\n",
    "            tmpHold[n] = np.array_split(grpIdx, nFolds) # split: N equals\n",
    "        chunks = [np.hstack(k) for k in zip(*tmpHold)] # concat sub chunks\n",
    "    else:\n",
    "        np.random.shuffle(idx) # random shuffle data\n",
    "        chunks = np.array_split(idx, nFolds) # split into N equal sized chunks\n",
    "\n",
    "    return chunks # return the prediction of the last fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoliPD = './data/ecoli.data'\n",
    "ecoliVars = ['seq', 'mcg', 'gvh', 'lip', 'chg', 'aac', 'alm1', 'alm2', 'class']\n",
    "################################################################################\n",
    "ecoliMat,ecoliFeat,ecoliY = prepData(ecoliPD, ecoliVars, slice(1,-1), '\\s+')\n",
    "folds = getCrossValidFolds(ecoliMat, ecoliY, categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def crossValidate(dataMat, labels, chunks, k, categ):\n",
    "    err = np.empty(len(chunks))\n",
    "    \n",
    "    for ck,idx in enumerate(chunks):\n",
    "        # get index and dataset for current fold of cross-validation\n",
    "        trnIdx = np.hstack([x for n,x in enumerate(folds) if n != ck])\n",
    "        vldIdx = np.hstack([x for n,x in enumerate(folds) if n == ck])\n",
    "        dataTrain,labelTrain = dataMat[trnIdx,:],labels[trnIdx] # training\n",
    "        dataTest,labelTest = dataMat[vldIdx,:],labels[vldIdx] # validation\n",
    "        \n",
    "        pred = KNN(dataTrain, labelTrain, dataTest, k)\n",
    "        err[ck] = errRate(pred, labelTest, categorical=categ)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def tuneK(dataMat, labels, chunks, categ):\n",
    "    Ks = np.arange(1,15)\n",
    "    err = np.empty(len(Ks))\n",
    "    \n",
    "    pick = np.random.randint(len(chunks)) # randomly pick fold as validation set\n",
    "    trnIdx = np.hstack([x for n,x in enumerate(folds) if n != pick]) # train\n",
    "    vldIdx = np.hstack([x for n,x in enumerate(folds) if n == pick]) # validate\n",
    "    dataTrain,labelTrain = dataMat[trnIdx,:],labels[trnIdx] # training\n",
    "    dataTest,labelTest = dataMat[vldIdx,:],labels[vldIdx] # validation\n",
    "    \n",
    "    for n,k in enumerate(Ks):\n",
    "        pred = KNN(dataTrain, labelTrain, dataTest, k)\n",
    "        err[n] = errRate(pred, labelTest, categorical=categ)\n",
    "    \n",
    "    return Ks[np.argmin(err)], err # return K with smallest error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "minK,errs = tuneK(ecoliMat, ecoliY, folds, categ=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12857143, 0.12857143, 0.15151515, 0.12307692, 0.12307692])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossValidate(ecoliMat, ecoliY, folds, minK, categ=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15714285714285714"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trnIdx = np.hstack([x for n,x in enumerate(folds) if n != 1])\n",
    "vldIdx = np.hstack([x for n,x in enumerate(folds) if n == 1])\n",
    "\n",
    "pred = KNN(ecoliMat[trnIdx], ecoliY[trnIdx], ecoliMat[vldIdx], 3)\n",
    "errRate(pred, ecoliY[vldIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.7531095   1.90570435  2.35356582  2.78923996  2.75831607  5.09819285\n",
      "   2.59454517  4.09085054 15.14762401 16.15038255 17.29086775 15.53368289\n",
      "  15.89654084]\n",
      " [ 2.89632479  2.42758659  1.78430884  3.09164191  3.69661392  4.88415513\n",
      "   2.20077493  2.93689178 16.01803308 16.85705599 18.08449835 16.2482904\n",
      "  16.6680666 ]\n",
      " [ 2.48400306  4.20228236  4.83424079  2.97053469  5.98471444  7.4226921\n",
      "   3.24846557  2.34355008 13.96268961 14.42267718 15.76245646 13.98130953\n",
      "  14.53907182]\n",
      " [ 4.05646107  3.21798525  3.57839308  4.03044589  6.29779001  6.9612301\n",
      "   4.18227002  3.3522941  16.50386363 17.35561302 18.59506191 16.63545198\n",
      "  17.17488197]\n",
      " [ 3.77493685  3.13927818  1.78213933  3.03847036  3.19118653  4.1723466\n",
      "   2.54653494  3.8257698  14.89907238 15.7502842  17.20118056 15.18789626\n",
      "  15.53537594]\n",
      " [15.62919977 14.65630948 16.17242659 14.01604968 14.99672879 17.4843315\n",
      "  15.2171927  15.29248839  2.15042781  2.21585641  4.3617274   1.96923112\n",
      "   2.08483425]\n",
      " [18.12475458 17.07915063 18.83218746 16.54213631 17.71978407 20.33204627\n",
      "  17.82679365 17.72507264  3.9036171   3.50309325  3.95423407  3.38762418\n",
      "   3.53974319]]\n"
     ]
    }
   ],
   "source": [
    "testX = np.vstack([np.random.randn(8,5)+9, np.random.randn(5,5)+2])\n",
    "testY = np.array(['B']*8 + ['S']*5)\n",
    "testPred = np.vstack([np.random.randn(5,5)+9, np.random.randn(2,5)+2])\n",
    "z = pairwiseDist(testPred, testX)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'B', 'B', 'B', 'B', 'S', 'S'], dtype='<U1')"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN(testX, testY, testPred, 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
