{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np, pandas as pd\n",
    "from utilities import discretizeMean, oneHot, normalizeDF, makeClassMat\n",
    "from crossValidate import getXVFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concateBias(x):\n",
    "    ''' Concatenate bias=1 term to input by adding a column of ones to the\n",
    "    right of the inut data. This is useful for training ANN bias.\n",
    "    '''\n",
    "    return np.hstack( [x, np.ones((x.shape[0],1), x.dtype)] )\n",
    "\n",
    "def sigmoid(x):\n",
    "    ''' Sigmoid function\n",
    "    '''\n",
    "    return 1.0 / ( 1 + np.exp(-x) )\n",
    "\n",
    "def softMax(x):\n",
    "    ''' Calculate normalized exponential function for weights and input matrix.\n",
    "    Returns the normalized exponential probabilities for each classes.\n",
    "    '''\n",
    "    pr = np.exp(x) # matrix multiply of x and weight\n",
    "    if pr.ndim > 1:\n",
    "        return pr / pr.sum(axis=1)[:,None]\n",
    "    else:\n",
    "        return pr / pr.sum()\n",
    "\n",
    "def crossEntNK(yhat, y):\n",
    "    ''' Cross Entropy function. Used as the error for multi-class ANN.\n",
    "    Calculate error across classes for all data points, and return mean err.\n",
    "    '''\n",
    "    return np.sum(-y*np.log(yhat), axis=1).mean() # avg error over all points\n",
    "\n",
    "def getRandomSeq(N):\n",
    "    seq = np.arange(N)\n",
    "    np.random.shuffle(seq)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisFile = os.path.join('./data/', 'iris.data')\n",
    "irisName = ['sepalLen', 'sepalWth', 'petalLen', 'petalWth', 'class']\n",
    "raw = pd.read_csv(irisFile , names=irisName)  # read CSV file\n",
    "irisTypes = makeClassMat(raw['class'])\n",
    "irisMat = normalizeDF(raw[irisName[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = getXVFolds(irisMat, irisTypes, categorical=True)\n",
    "testIdx = folds[0]\n",
    "trainIdx = np.hstack(folds[1:])\n",
    "trainData,trainLabel = irisMat[trainIdx],irisTypes[trainIdx]\n",
    "testData,testLabel = irisMat[testIdx],irisTypes[testIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def train_0hidd(xMat, yMat, eta, eps=1e-7, trace=False, shuffle=True):\n",
    "    def feedForward(xs, ys, wts):\n",
    "        return softMax(xs @ wts)\n",
    "    \n",
    "    def backProp(ys, yfit, xs, wts):\n",
    "        return wts + eta * np.outer(xs, ys-yfit)\n",
    "    \n",
    "    xMat = concateBias(xMat) # add bias terms\n",
    "    (nData,nK),nDim = yMat.shape, xMat.shape[1] # size of data and classes\n",
    "    \n",
    "    wt = np.random.rand(nDim,nK)/50 - 0.01 # init wts to be (-0.01,0.01)\n",
    "    lastErr = np.inf # max error possible\n",
    "    yHats = feedForward(xMat, yMat, wt) # first feedforward calc\n",
    "    meanErr = crossEntNK(yHats, yMat) # error from random weights\n",
    "    \n",
    "    epch = 0\n",
    "    while (abs(meanErr-lastErr) > eps) and epch < 1e6: # while not converged\n",
    "        if epch%1000==0 and trace:\n",
    "            print('Iter #%u, error: %f'%(epch,meanErr))\n",
    "        \n",
    "        if shuffle: # shuffle sequence of gradient descent\n",
    "            seq = getRandomSeq(nData) # random seq for stoch. gradient descent\n",
    "        else:\n",
    "            seq = np.arange(nData)\n",
    "        for n in seq: # loop over data set\n",
    "            x,y = xMat[n],yMat[n] # index x and y for curr data point\n",
    "            yHat = feedForward(x, y, wt) # feedforward\n",
    "            wt = backProp(y, yHat, x, wt) # update weight\n",
    "        \n",
    "        lastErr = meanErr\n",
    "        yHats = feedForward(xMat, yMat, wt) # fitted Y for this epoch\n",
    "        meanErr = crossEntNK(yHats, yMat) # err for this epoch\n",
    "        \n",
    "        if meanErr > lastErr:  # slow learning rate if error increase\n",
    "            eta /= 5\n",
    "        epch += 1\n",
    "\n",
    "    if trace: # print final error\n",
    "        print('Final iteration #%u, error: %f' % (epch-1,meanErr) )\n",
    "    return wt,epch\n",
    "\n",
    "def pred_0hidd(xMat, wts):\n",
    "    yHat = softMax(concateBias(testData) @ wt)\n",
    "    return yHat.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05876141 0.04282027 0.06751971 0.0436813  0.04986794 0.03387666]\n",
      "[0.05876141 0.04282027 0.06751971 0.0436813  0.04986794 0.03387666]\n"
     ]
    }
   ],
   "source": [
    "v = np.random.rand(6,4)/10\n",
    "print(np.sum(testData[0] * v, axis=1))\n",
    "print(v @ testData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def train_1hidd(xMat, yMat, eta, nNodes, eps=1e-7, trace=False, shuffle=True):\n",
    "    def feedForward(xs, ys, wtsOut, wtsHidd):\n",
    "        zs = sigmoid(xs @ wtsHidd)\n",
    "        return zs, softMax(zs @ wtsOut)\n",
    "    \n",
    "    def backProp(ys, yfit, xs, zs, wtsOut, wtsHidd):\n",
    "        d_Out = eta * np.outer(zs, ys-yfit)\n",
    "        d_hidd = eta * np.outer(xs, (wtsOut@(ys-yfit)) * zs*(1-zs))\n",
    "        return wtsOut + d_Out, wtsHidd + d_hidd\n",
    "    \n",
    "    xMat = concateBias(xMat)\n",
    "    (nData,nK),nDim = yMat.shape, xMat.shape[1]\n",
    "    \n",
    "    wtOut = np.random.rand(nNodes,nK)/50 - 0.01 # init wts to be (-0.01,0.01)\n",
    "    wtHidd = np.random.rand(nDim,nNodes)/50 - 0.01\n",
    "    \n",
    "    lastErr = np.inf # max error possible\n",
    "    zs,yHats = feedForward(xMat, yMat, wtOut, wtHidd)\n",
    "    meanErr = crossEntNK(yHats, yMat)\n",
    "    \n",
    "    epch = 0\n",
    "    while (abs(meanErr-lastErr) > eps) and epch < 1e6: # while not converged\n",
    "        if epch%1000==0 and trace:\n",
    "            print('Iter #%u, error: %f'%(epch,meanErr))\n",
    "        \n",
    "        if shuffle:\n",
    "            seq = getRandomSeq(nData) # random seq for stoch. gradient descent\n",
    "        else:\n",
    "            seq = np.arange(nData)\n",
    "        for n in seq: # loop over data set\n",
    "            x,y = xMat[n],yMat[n] # index x and y for curr data point\n",
    "            z,yHat = feedForward(x, y, wtOut, wtHidd) # feedforward\n",
    "            wtOut,wtHidd = backProp(y, yHat, x, z, wtOut, wtHidd) # update weight\n",
    "################################################################################\n",
    "        lastErr = meanErr\n",
    "        zs,yHats = feedForward(xMat, yMat, wtOut, wtHidd) # fitted Y for this epoch\n",
    "        meanErr = crossEntNK(yHats, yMat) # err for this epoch\n",
    "        \n",
    "        if meanErr > lastErr:  # slow learning rate if error increase\n",
    "            eta /= 2\n",
    "        epch += 1\n",
    "\n",
    "    if trace: # print final error\n",
    "        print('Final iteration #%u, error: %f' % (epch-1,meanErr) )\n",
    "    return (wtHidd,wtOut),epch\n",
    "\n",
    "def pred_1hidd(xMat, wtsHidd, wtsOut):\n",
    "    yHat = softMax(sigmoid(concateBias(xMat) @ wtsHidd) @ wtsOut)\n",
    "    return yHat.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def train_hidd(xMat, yMat, eta, nLayers, nNodesHidd, eps=1e-7, trace=False, \n",
    "               shuffle=True):\n",
    "    def feedForward(xs, ys, wtsOut, wtsHidd):\n",
    "        zs = sigmoid(xs @ wtsHidd)\n",
    "        return zs, softMax(zs @ wtsOut)\n",
    "    \n",
    "    def backProp(ys, yfit, xs, zs, wtsOut, wtsHidd):\n",
    "        d_Out = eta * np.outer(zs, ys-yfit)\n",
    "        d_hidd = eta * np.outer(xs, (wtsOut@(ys-yfit)) * zs*(1-zs))\n",
    "        return wtsOut + d_Out, wtsHidd + d_hidd\n",
    "    \n",
    "    def initWeights(D, K, nHidd, layers, )\n",
    "    \n",
    "    xMat = concateBias(xMat)\n",
    "    (nData,nK),nDim = yMat.shape, xMat.shape[1]\n",
    "    \n",
    "    wtOut = np.random.rand(nNodesHidd,nK)/50 - 0.01 # init wts to be (-0.01,0.01)\n",
    "    wtHidd = np.random.rand(nDim,nNodesHidd)/50 - 0.01\n",
    "    \n",
    "    lastErr = np.inf # max error possible\n",
    "    zs,yHats = feedForward(xMat, yMat, wtOut, wtHidd)\n",
    "    meanErr = crossEntNK(yHats, yMat)\n",
    "    \n",
    "    epch = 0\n",
    "    while (abs(meanErr-lastErr) > eps) and epch < 1e6: # while not converged\n",
    "        if epch%1000==0 and trace:\n",
    "            print('Iter #%u, error: %f'%(epch,meanErr))\n",
    "        \n",
    "        if shuffle:\n",
    "            seq = getRandomSeq(nData) # random seq for stoch. gradient descent\n",
    "        else:\n",
    "            seq = np.arange(nData)\n",
    "        for n in seq: # loop over data set\n",
    "            x,y = xMat[n],yMat[n] # index x and y for curr data point\n",
    "            z,yHat = feedForward(x, y, wtOut, wtHidd) # feedforward\n",
    "            wtOut,wtHidd = backProp(y, yHat, x, z, wtOut, wtHidd) # update weight\n",
    "################################################################################\n",
    "        lastErr = meanErr\n",
    "        zs,yHats = feedForward(xMat, yMat, wtOut, wtHidd) # fitted Y for this epoch\n",
    "        meanErr = crossEntNK(yHats, yMat) # err for this epoch\n",
    "        \n",
    "        if meanErr > lastErr:  # slow learning rate if error increase\n",
    "            eta /= 2\n",
    "        epch += 1\n",
    "\n",
    "    if trace: # print final error\n",
    "        print('Final iteration #%u, error: %f' % (epch-1,meanErr) )\n",
    "    return (wtHidd,wtOut),epch\n",
    "\n",
    "def pred_1hidd(xMat, wtsHidd, wtsOut):\n",
    "    yHat = softMax(sigmoid(concateBias(xMat) @ wtsHidd) @ wtsOut)\n",
    "    return yHat.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #0, error: 1.098616\n",
      "Final iteration #460, error: 0.047875\n"
     ]
    }
   ],
   "source": [
    "wt,nn = train_1hidd(trainData, trainLabel, 2, 8, eps=1e-6, trace=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_1hidd(testData, *wt) == testLabel.argmax(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
