{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np, pandas as pd\n",
    "from utilities import discretizeMean, oneHot, normalizeDF, makeClassMat\n",
    "from crossValidate import getXVFolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concateBias(x):\n",
    "    ''' Concatenate bias=1 term to input by adding a column of ones to the\n",
    "    right of the inut data. This is useful for training ANN bias.\n",
    "    '''\n",
    "    return np.hstack( [x, np.ones((x.shape[0],1), x.dtype)] )\n",
    "\n",
    "def sigmoid(x):\n",
    "    ''' Sigmoid function\n",
    "    '''\n",
    "    return 1.0 / ( 1 + np.exp(-x) )\n",
    "\n",
    "def softMax(x):\n",
    "    ''' Calculate normalized exponential function for weights and input matrix.\n",
    "    Returns the normalized exponential probabilities for each classes.\n",
    "    '''\n",
    "    pr = np.exp(x) # matrix multiply of x and weight\n",
    "    if pr.ndim > 1:\n",
    "        return pr / pr.sum(axis=1)[:,None]\n",
    "    else:\n",
    "        return pr / pr.sum()\n",
    "\n",
    "def crossEntNK(yhat, y):\n",
    "    ''' Cross Entropy function. Used as the error for multi-class ANN.\n",
    "    Calculate error across classes for all data points, and return mean err.\n",
    "    '''\n",
    "    return np.sum(-y*np.log(yhat), axis=1).mean() # avg error over all points\n",
    "\n",
    "def getRandomSeq(N):\n",
    "    seq = np.arange(N)\n",
    "    np.random.shuffle(seq)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisFile = os.path.join('./data/', 'iris.data')\n",
    "irisName = ['sepalLen', 'sepalWth', 'petalLen', 'petalWth', 'class']\n",
    "raw = pd.read_csv(irisFile , names=irisName)  # read CSV file\n",
    "irisTypes = makeClassMat(raw['class'])\n",
    "irisMat = normalizeDF(raw[irisName[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = getXVFolds(irisMat, irisTypes, categorical=True)\n",
    "testIdx = folds[0]\n",
    "trainIdx = np.hstack(folds[1:])\n",
    "trainData,trainLabel = irisMat[trainIdx],irisTypes[trainIdx]\n",
    "testData,testLabel = irisMat[testIdx],irisTypes[testIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def train_0hidd(xMat, yMat, eta, eps=1e-7, trace=False, stochastic=True):\n",
    "    def feedForward(xs, ys, wts):\n",
    "        return softMax(xs @ wts)\n",
    "    \n",
    "    def backProp(ys, yfit, xs, wts):\n",
    "        return wts + eta * np.outer(xs, ys-yfit)\n",
    "    \n",
    "    xMat = concateBias(xMat)\n",
    "    (nData,nK),nDim = yMat.shape, xMat.shape[1]\n",
    "    \n",
    "    wt = np.random.rand(nDim,nK)/50 - 0.01 # init wts to be (-0.01,0.01)\n",
    "    lastErr = np.inf # max error possible\n",
    "    yHats = feedForward(xMat, yMat, wt)\n",
    "    meanErr = crossEntNK(yHats, yMat)\n",
    "    \n",
    "    epch = 0\n",
    "    while (abs(meanErr-lastErr) > eps) and epch < 1e6: # while not converged\n",
    "        if epch%1000==0 and trace:\n",
    "            print('Iter #%u, error: %f'%(epch,meanErr))\n",
    "        \n",
    "        if stochastic:\n",
    "            seq = getRandomSeq(nData) # random seq for stoch. gradient descent\n",
    "        else:\n",
    "            seq = np.arange(nData)\n",
    "        for n in seq: # loop over data set\n",
    "            x,y = xMat[n],yMat[n] # index x and y for curr data point\n",
    "        #for x,y in zip(xMat,yMat):\n",
    "            yHat = feedForward(x, y, wt) # feedforward\n",
    "            wt = backProp(y, yHat, x, wt) # update weight\n",
    "        \n",
    "        lastErr = meanErr\n",
    "        yHats = feedForward(xMat, yMat, wt) # fitted Y for this epoch\n",
    "        meanErr = crossEntNK(yHats, yMat) # err for this epoch\n",
    "        \n",
    "        if meanErr > lastErr:  # slow learning rate if error increase\n",
    "            eta /= 5\n",
    "        epch += 1\n",
    "\n",
    "    if trace: # print final error\n",
    "        print('Final iteration #%u, error: %f' % (epch-1,meanErr) )\n",
    "    return wt,epch\n",
    "\n",
    "def pred_0hidd(xMat, wts):\n",
    "    yHat = softMax(concateBias(testData) @ wt)\n",
    "    return yHat.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def train_1hidd(xMat, yMat, eta, nHidd, eps=1e-7, trace=False, stochastic=True):\n",
    "    def feedForward(xs, ys, wts):\n",
    "        return softMax(xs @ wts)\n",
    "    \n",
    "    def backProp(ys, yfit, xs, wts):\n",
    "        return wts + eta * np.outer(xs, ys-yfit)\n",
    "    \n",
    "    xMat = concateBias(xMat)\n",
    "    (nData,nK),nDim = yMat.shape, xMat.shape[1]\n",
    "    \n",
    "    wtOut = np.random.rand(nDim,nK)/50 - 0.01 # init wts to be (-0.01,0.01)\n",
    "    wtHidd = \n",
    "    lastErr = np.inf # max error possible\n",
    "    yHats = feedForward(xMat, yMat, wt)\n",
    "    meanErr = crossEntNK(yHats, yMat)\n",
    "    \n",
    "    epch = 0\n",
    "    while (abs(meanErr-lastErr) > eps) and epch < 1e6: # while not converged\n",
    "        if epch%1000==0 and trace:\n",
    "            print('Iter #%u, error: %f'%(epch,meanErr))\n",
    "        \n",
    "        if stochastic:\n",
    "            seq = getRandomSeq(nData) # random seq for stoch. gradient descent\n",
    "        else:\n",
    "            seq = np.arange(nData)\n",
    "        for n in seq: # loop over data set\n",
    "            x,y = xMat[n],yMat[n] # index x and y for curr data point\n",
    "        #for x,y in zip(xMat,yMat):\n",
    "            yHat = feedForward(x, y, wt) # feedforward\n",
    "            wt = backProp(y, yHat, x, wt) # update weight\n",
    "        \n",
    "        lastErr = meanErr\n",
    "        yHats = feedForward(xMat, yMat, wt) # fitted Y for this epoch\n",
    "        meanErr = crossEntNK(yHats, yMat) # err for this epoch\n",
    "        \n",
    "        if meanErr > lastErr:  # slow learning rate if error increase\n",
    "            eta /= 2\n",
    "        epch += 1\n",
    "\n",
    "    if trace: # print final error\n",
    "        print('Final iteration #%u, error: %f' % (epch-1,meanErr) )\n",
    "    return wt,epch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #0, error: 1.099841\n",
      "Iter #1000, error: 0.131205\n",
      "Iter #2000, error: 0.111484\n",
      "Iter #3000, error: 0.099544\n",
      "Iter #4000, error: 0.091456\n",
      "Iter #5000, error: 0.085571\n",
      "Iter #6000, error: 0.081075\n",
      "Iter #7000, error: 0.077514\n",
      "Iter #8000, error: 0.074614\n",
      "Iter #9000, error: 0.072203\n",
      "Iter #10000, error: 0.070162\n",
      "Iter #11000, error: 0.068410\n",
      "Iter #12000, error: 0.066888\n",
      "Iter #13000, error: 0.065553\n",
      "Iter #14000, error: 0.064370\n",
      "Final iteration #14029, error: 0.064337\n"
     ]
    }
   ],
   "source": [
    "wt,nn = train_0hidd(trainData, trainLabel, 0.5, eps=1e-6, trace=True, stochastic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_0hidd(testData, wt) == testLabel.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLabel.ndim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
