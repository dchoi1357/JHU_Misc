{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "There are general instructions on Blackboard and in the Syllabus for Programming Assignments. This Notebook also has instructions specific to this assignment. Read all the instructions carefully and make sure you understand them. Please ask questions on the discussion boards or email me at `EN605.445@gmail.com` if you do not understand something.\n",
    "\n",
    "<div style=\"background: mistyrose; color: firebrick; border: 2px solid darkred; padding: 5px; margin: 10px;\">\n",
    "You must follow the directions *exactly* or you will get a 0 on the assignment.\n",
    "</div>\n",
    "\n",
    "You must submit a zip file of your assignment and associated files (if there are any) to Blackboard. The zip file will be named after you JHED ID: `<jhed_id>.zip`. It will not include any other information. Inside this zip file should be the following directory structure:\n",
    "\n",
    "```\n",
    "<jhed_id>\n",
    "    |\n",
    "    +--module-04-programming.ipynb\n",
    "    +--module-04-programming.html\n",
    "    +--world.txt\n",
    "    +--test01.txt\n",
    "    +--(any other files)\n",
    "```\n",
    "\n",
    "For example, do not name  your directory `programming_assignment_01` and do not name your directory `smith122_pr1` or any else. It must be only your JHED ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import *\n",
    "from StringIO import StringIO\n",
    "import copy, sys, random, math\n",
    "\n",
    "# add whatever else you need from the Anaconda packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Q-Learning\n",
    "\n",
    "The world for this problem is very similar to the one from Module 1 that we solved with A\\* search but this time we're going to use a different approach.\n",
    "\n",
    "We're replacing the deterministic movement with stochastic movement. This means, when the agent moves \"south\" instead of always going \"south\", there is a probability distribution of possible successor states \"south\", \"east\", \"north\" and \"west\". Thus we may not end up in the state we planned!\n",
    "\n",
    "There are a variety of ways to handle this problem. For example, if using A\\* search, if the agent finds itself off the solution, you can simply calculate a new solution from where the agent ended up. Although this sounds like a really bad idea, it has actually been shown to work really well in Video Games that use formal Planning algorithms (which we will cover later). When these algorithms were first designed, this was unthinkable. Thank you, Moore's Law!\n",
    "\n",
    "Another approach is to use Reinforcement Learning which covers problems where there is some kind of general uncertainty. We're going to model that uncertainty a bit unrealistically here but it'll show you how the algorithm works.\n",
    "\n",
    "As far as RL is concerned, there are a variety of options there: model-based and model-free, Value Iteration, Q-Learning and SARSA. You are going to use Value Iteration and Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The World Representation\n",
    "\n",
    "As before, we're going to simplify the problem by working in a grid world. The symbols that form the grid have a special meaning as they specify the type of the terrain and the cost to enter a grid cell with that type of terrain:\n",
    "\n",
    "```\n",
    "token   terrain    cost \n",
    ".       plains     1\n",
    "*       forest     3\n",
    "^       hills      5\n",
    "~       swamp      7\n",
    "x       mountains  impassible\n",
    "```\n",
    "\n",
    "When you go from a plains node to a forest node it costs 3. When you go from a forest node to a plains node, it costs 1. You can think of the grid as a big graph. Each grid cell (terrain symbol) is a node and there are edges to the north, south, east and west (except at the edges).\n",
    "\n",
    "There are quite a few differences between A\\* Search and Reinforcement Learning but one of the most salient is that A\\* Search returns a plan of N steps that gets us from A to Z, for example, A->C->E->G.... Reinforcement Learning, on the other hand, returns  a *policy* that tells us the best thing to do **for every and any state.**\n",
    "\n",
    "For example, the policy might say that the best thing to do in A is go to C. However, we might find ourselves in D instead. But the policy covers this possibility, it might say, D->E. Trying this action might land us in C and the policy will say, C->E, etc. At least with offline learning, everything will be learned in advance (in online learning, you can only learn by doing and so you may act according to a known but suboptimal policy).\n",
    "\n",
    "Nevertheless, if you were asked for a \"best case\" plan from (0, 0) to (n-1, n-1), you could (and will) be able to read it off the policy because there is a best action for every state. You will be asked to provide this in your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Map\n",
    "\n",
    "To avoid global variables, we have a <code>read_world()</code> function that takes a filename and returns the world as `List` of `List`s. **The same coordinates reversal applies: (x, y) is world[ y][ x] as from PR01.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_world( filename):\n",
    "    with open( filename, 'r') as f:\n",
    "        world_data = [x for x in f.readlines()]\n",
    "    f.closed\n",
    "    world = []\n",
    "    for line in world_data:\n",
    "        line = line.strip()\n",
    "        if line == \"\": continue\n",
    "        world.append([x for x in line])\n",
    "    return world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a dict of movement costs. Note that we've negated them this time because RL requires negative costs and positive rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'*': -3, '.': -1, '^': -5, '~': -7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs = { '.': -1, '*': -3, '^': -5, '~': -7}\n",
    "costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a list of offsets for `cardinal_moves`. You'll need to work this into your actions, A, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cardinal_moves = [(0,-1), (1,0), (0,1), (-1,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the confusing bits begin. We must program both the Q-Learning algorithm and a *simulator*. The Q-Learning algorithm doesn't know T but the simulator *must*. Essentially the *simulator* is any time you apply a move and check to see what state you actually end up in (rather than the state you planned to end up in).\n",
    "\n",
    "The transition function your *simulation* should use, T, is 0.70 for the desired direction, and 0.10 each for the other possible directions. That is, if I select \"up\" then 70% of the time, I go up but 10% of the time I go left, 10% of the time I go right and 10% of the time I go down. If you're at the edge of the map, you simply bounce back to the current state.\n",
    "\n",
    "You need to implement `q_learning()` with the following parameters:\n",
    "\n",
    "+ world: a `List` of `List`s of terrain (this is S from S, A, T, gamma, R)\n",
    "+ costs: a `Dict` of costs by terrain (this is part of R)\n",
    "+ goal: A `Tuple` of (x, y) stating the goal state.\n",
    "+ reward: The reward for achieving the goal state.\n",
    "+ actions: a `List` of possible actions, A, as offsets.\n",
    "+ gamma: the discount rate\n",
    "+ alpha: the learning rate\n",
    "\n",
    "you will return a policy: \n",
    "\n",
    "`{(x1, y1): action1, (x2, y2): action2, ...}`\n",
    "\n",
    "Remember...a policy is what to do in any state for all the states. Notice how this is different that A\\* search which only returns actions to take from the start to the goal. This also explains why `q_learning` doesn't take a `start` state!\n",
    "\n",
    "You should also define a function `pretty_print_policy( cols, rows, policy)` that takes a policy and prints it out as a grid using \"^\" for up, \"<\" for left, \"v\" for down and \">\" for right. Note that it doesn't need the `world` because the policy has a move for every state. However, you do need to know how big the grid is so you can pull the values out of the `Dict` that is returned.\n",
    "\n",
    "```\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    ">>>>>>v\n",
    "^^^>>>v\n",
    "^^^>>>v\n",
    "^^^>>>G\n",
    "```\n",
    "\n",
    "(Note that that policy is completely made up and only illustrative of the desired output).\n",
    "\n",
    "There are a lot of details that I have left up to you. For example, when do you stop? Is there a strategy for learning the policy? Watch and re-watch the lecture on Q-Learning. Ask questions. You need to implement a way to pick initial states for each iteration and you need a way to balance exploration and exploitation while learning. You may have to experiment with different gamma and alpha values. Be careful with your reward...the best reward is related to the discount rate and the approxmiate number of actions you need to reach the goal.\n",
    "\n",
    "* If everything is otherwise the same, do you think that the path from (0,0) to the goal would be the same for both A\\* Search and Q-Learning?\n",
    "* What do you think if you have a map that looks like:\n",
    "\n",
    "```\n",
    "><>>^\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>G\n",
    "```\n",
    "\n",
    "has this converged? Is this a \"correct\" policy?\n",
    "\n",
    "**I strongly suggest that you implement Value Iteration on your own to solve these problems.** Why? Because Value Iteration will find the policy to which Q-Learning should coverge in the limit. If you include your Value Iteration implementation and output, I will count it towards your submission grade.\n",
    "\n",
    "Remember that you should follow the general style guidelines for this course: well-named, short, focused functions with limited indentation using Markdown documentation that explains their implementation and the AI concepts behind them.\n",
    "\n",
    "This assignment sometimes wrecks havoc with IPython notebook, save often. Put your helper functions here, along with their documentation. There should be one Markdown cell for the documentation, followed by one Codecell for the implementation.  Additionally, you may want to start your programming in a regular text editor/IDE because RL **takes a long time to run**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Helper Functions ###\n",
    "The following functions are used by both Q-learning and Value Iteration.\n",
    "\n",
    "** transCosts(world, moves, costs, goal, reward) **  \n",
    "The function produces all valid transitions from all locations/states as well as the cost/reward for said transitions.  \n",
    "\n",
    "The input to the functions are: world represented as list of lists of terrains as represented by strings, the entire moveset as cartesian offsets, the costs as a dictionary where the keys are the terrains as strings, the goal as a tuple of list indices, and reward as a int \n",
    "\n",
    "The function loops through all states, and for every valid states (i.e. passible and non-goal states), attemps all actions in the `moves` input from the state. Only valid transitions are added to the output, meaning that a transition onto impassible terrain or out of bound are not returned. For all transition, a corresponding cost/reward is attached to all transitions either as the cost of the destination terrain, or the reward if transition from the state results in the goal.\n",
    "\n",
    "The output is a list of list, where the indices represent the indices of the world. For example, `R[0][4] = [((0, 1), -1), ((-1, 0), 9)]` means that from the state of `[0][4]`, one could either move up (cartesian offset of (0,1)) incurring a cost of 1, or move left, earning a reward of 9. \n",
    "\n",
    "** getPolicyFromQ(Q, Rs) **  \n",
    "The function takes a set Q-values and the corresponding transitions, and returns both the maximum Q-value and the corresponding transition. The input could be thought of as set of transitions from a specific state, and the Q-value of these transitions. The function finds the maximum Q-value and the move that produce this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transCosts(world, moves, costs, goal, reward):\n",
    "    R = [[-9 for c in r] for r in world] \n",
    "\n",
    "    for r,row in enumerate(world):\n",
    "        for c,terr in enumerate(row):\n",
    "            if (world[r][c]=='x') or ((r,c) == goal):\n",
    "                continue # skip impassible or goal state\n",
    "\n",
    "            acts = list()            \n",
    "            for m,offset in enumerate(moves):\n",
    "                x,y = (r+offset[1], c+offset[0]) # new coordinates\n",
    "                if (0<=x<len(world) and 0<=y<len(row)) and (world[x][y] != 'x'):\n",
    "                    #acts.append( (offset, costs[world[r][c]]) )\n",
    "                    if (x,y) == goal:\n",
    "                        acts.append((offset, reward ))\n",
    "                    else:\n",
    "                        acts.append((offset, costs[world[x][y]] ))\n",
    "            R[r][c] = acts\n",
    "\n",
    "    R[goal[0]][goal[1]] = [0]\n",
    "    return R\n",
    "\n",
    "def getPolicyFromQ(Q, Rs):\n",
    "    moves = [m for m,r in Rs] # all moves\n",
    "    V = max(Q) # max reward of all moves\n",
    "    policy = moves[Q.index(V)] # mark policy of the max reward\n",
    "    return (V, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** pretty_print_policy(world, goal, policy) **  \n",
    "The function takes a policy as a dictionary and prints the policy as a rectangular representation where each location represent the \"best move\" from the location. The dictionary representing the policy has keys that are indices to the list of list representing the world, and the values are the cartesian offset to these indices. \n",
    "\n",
    "The function loops over every location in the world and printing the directional symbol as indicated by the policy. For impassible and goal locations, the functions prints `x` and `G` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_print_policy(world, goal, policy):\n",
    "    m = {(0,1):'v', (1,0):'>', (0,-1):'^', (-1,0):'<', -1:'x'}\n",
    "    out = copy.deepcopy(world) # pre-allocate\n",
    "    \n",
    "    for r,row in enumerate(world):\n",
    "        for c,terrain in enumerate(row):\n",
    "            out[r][c] = m[policy.get((r,c),-1)] # if cannot find mark as impassible\n",
    "    out[goal[0]][goal[1]] = 'G' # mark the goal\n",
    "\n",
    "    for row in out:\n",
    "        print('\\t' + ''.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Helper Functions ###\n",
    "These functions are only used by Q-Learning Program\n",
    "\n",
    "** pickStartingState(world, goal) **  \n",
    "The function picks a random starting point, and returns the first valid one found (i.e. passible and non-goal state).\n",
    "\n",
    "** initializeQs(Ts) **  \n",
    "Given a set of transitions for every state, the function initializes a corresponding Q-value set for all states and all transitions for each of the state, setting them all to 0. This is used to initialize Q for the Q-Learning program.\n",
    "\n",
    "** randomizeAction(nActs, desired, unplanned) **  \n",
    "Given the number of valid actions, the index of the desired action, the probability of resulting in an unplanned transition, the function probabilistically return the index of the action taken.\n",
    "\n",
    "The `unplanned` represents the probability of taking one singular undesired action. Depending on the number of valid action, the probability of picking the desired action is (1 - Pr(unplanned) * (number of valid actions - 1)). If only one valid action is present, the function always take the singular action.\n",
    "\n",
    "** getDesiredAction(Q, Ts, visits) **  \n",
    "The E-Greedy algorithm that determines which action is taken from a specific state. Hard-coded 50% probability of picking a random action, with 50% of picking based on the current Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickStartingState(world, goal): # pick start randomly\n",
    "    while True:\n",
    "        r = random.randrange(len(world))\n",
    "        c = random.randrange(len(world[r]))\n",
    "        if (world[r][c]!='x') and ((r,c)!=goal):\n",
    "            return (r,c)\n",
    "        \n",
    "def initializeQs(Ts):\n",
    "    Qs = copy.deepcopy(Ts)\n",
    "    for r,row in enumerate(Ts):\n",
    "        for c,acts in enumerate(row):\n",
    "            if type(acts) is int:\n",
    "                continue\n",
    "            else:\n",
    "                Qs[r][c] = [0 for a in acts]\n",
    "    return Qs\n",
    "\n",
    "def randomizeAction(nActs, desired, unplanned):\n",
    "    if nActs == 1:\n",
    "        return desired\n",
    "    \n",
    "    undesired = [x for x in xrange(nActs) if x!=desired]\n",
    "    r = random.random()\n",
    "    thresh = unplanned * (nActs-1)\n",
    "    if r > thresh:\n",
    "        return desired\n",
    "    else:\n",
    "        ind = int(math.floor(r / thresh * (nActs-1)))\n",
    "        return undesired[ind]\n",
    "\n",
    "def getDesiredAction(Q, Ts, visits):\n",
    "    if random.random() < 0.5:\n",
    "        return random.randrange(len(Q)) # randomly pick desired action\n",
    "    else:\n",
    "        return Q.index(max(Q)) # pick based on largest Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Program ###\n",
    "\n",
    "The program is split into two functions, where `q_learning(...)` is the overall Q-Learning program, and `q_episode(...)` simulates a singular episode of Q-Learning.\n",
    "\n",
    "** q_episode(Qs, Ts, a, g, pr, world, goal) **  \n",
    "Runs one episode of Q-Learning. The function simulates one trip from a randomized starting state to a goal state.\n",
    "\n",
    "From the starting state, the function uses E-Greedy algorithm with 50% chance of picking based by current Q-value, and 50% randomly. For the chosen state, the function then updates the `Q[s,a]` according to the algorithm, using the set of transition generated by `transCosts()` function. The function stops when the current state is a goal state.\n",
    "\n",
    "** q_learning( world, costs, goal, reward, actions, gamma, alpha) **  \n",
    "This is the main function of Q-Learning program. The function starts off by generating all valid transitions for all states in the world. It then initializes set of Qs for these state-transition combinations. With a hard-coded unplanned action probability of 0.1, the program performs 1000 episodes of Q-learning using the `q_episode(...)` function. With the generated Q, the function then determines the policy based on these Q-values by looping through all valid states using `getPolicyFromQ(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_episode(Qs, Ts, a, g, pr, world, goal):\n",
    "    visits = [[0 for x in row] for row in world] # visit count\n",
    "    (r, c) = pickStartingState(world, goal) # pick starting poisition\n",
    "    \n",
    "    while (r,c) != goal:\n",
    "        visits[r][c] += 1 # increment visit count for start position\n",
    "        nActions = len(Ts[r][c])\n",
    "        desired = getDesiredAction(Qs[r][c], Ts[r][c], visits)\n",
    "        actual = randomizeAction(nActions, desired, pr)\n",
    "        offset,reward = Ts[r][c][actual]\n",
    "\n",
    "        r_new, c_new = (r+offset[1], c+offset[0])\n",
    "        maxQ = max(Qs[r_new][c_new])\n",
    "        Qs[r][c][desired] = (1-a)*Qs[r][c][desired] + a * (reward + g*maxQ)\n",
    "        r,c = (r_new, c_new) # update the state\n",
    "        \n",
    "    return Qs\n",
    "    \n",
    "def q_learning( world, costs, goal, reward, actions, gamma, alpha):\n",
    "    Ts = transCosts(world, actions, costs, goal, reward) # unk. transitions\n",
    "    Qs = initializeQs(Ts)\n",
    "    \n",
    "    P_unplan = 0.1\n",
    "    \n",
    "    t = 0\n",
    "    while t < 1000:\n",
    "        Qs = q_episode( Qs, Ts, alpha, gamma, P_unplan, world, goal)\n",
    "        t += 1\n",
    "    \n",
    "    policy = dict()\n",
    "    for r,row in enumerate(world):\n",
    "        for c,terr in enumerate(row):\n",
    "            if (world[r][c]=='x') or ((r,c) == goal):\n",
    "                continue # skip impassible or goal state\n",
    "            tmp, policy[(r,c)] = getPolicyFromQ(Qs[r][c], Ts[r][c])\n",
    "    \n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.', 'x', '.', '.', '.', 'x'],\n",
       " ['.', '^', '.', '*', '.', '^'],\n",
       " ['.', '^', '.', '^', '.', '*'],\n",
       " ['.', '^', '.', '^', '.', '*'],\n",
       " ['.', '.', '.', 'x', '.', '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_world = read_world( \"small.txt\")\n",
    "test_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward = 100\n",
    "goal = (4,5)\n",
    "disc = 0.85\n",
    "alpha = 0.25\n",
    "\n",
    "test_policy = q_learning(test_world, costs, goal, reward, cardinal_moves, disc, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvx>>vx\n",
      "\tv>v>vv\n",
      "\tv>>v>v\n",
      "\tv>>>vv\n",
      "\t>>^x>G\n"
     ]
    }
   ],
   "source": [
    "pretty_print_policy(test_world, goal, test_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_world = read_world( \"world.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = 2000\n",
    "goal = (26,26)\n",
    "disc = 0.7\n",
    "alpha = 0.25\n",
    "\n",
    "full_policy = q_learning(full_world, costs, goal, reward, cardinal_moves, disc, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvvv<vvv<<<<>>>>>>>>>><<<<><\n",
      "\t^<^<<<<<<vvv>>>>^^xxxxxxx>^\n",
      "\t^^^<xx^<<v<<<>>^^xxxv<vxx^^\n",
      "\t^>^^<xxx>vv<<^^>>>vvv<<xx>v\n",
      "\tv><^^xx>>vv<<>>^^vvvvvxxx>v\n",
      "\tv<^^xx>v>vv<<<>^>>^><<<x>>v\n",
      "\t^<<xx>>>v<v<xxx^^>>>^<<>>>v\n",
      "\tv<v<>>>^^<^<<<xxxv^v><<v>vv\n",
      "\tv<<<<>>>^<><<^xxv<>v<^vv>>v\n",
      "\t^^<^^>>v^^^xxxxv<v>^<^>>vvv\n",
      "\t^^^<>^^^<^xxx>>>^<vvxxx>vvv\n",
      "\t^^<<vv>^<xx>v<^^<>><<xxv^>v\n",
      "\t^<vv>><<<xxv^v^^>>>v<x>v>vv\n",
      "\tv<vv>>v<vv>><<<<^v>^<<><vvv\n",
      "\tv<<<x>^<<<>^^^^<vv>v<<x>vvv\n",
      "\tv><xxx^<<<^vxxx^>v<^^xx>>>v\n",
      "\tvvxx>>^><^>vvvxxx><xxxvv>vv\n",
      "\t><<xx>^^^>v>>>vvxxxx>>>>>>v\n",
      "\t^^^xxx^<^v>>v>v<<<>>>>>>>vv\n",
      "\tv<^<xxx>>v>>>>>vv>vv>^^vvvv\n",
      "\t^^<v<vxx>>>>>^x>>>vv<>>vv>v\n",
      "\t^^vvvvvxxx^^xx>>^^<^vv>vv>v\n",
      "\t^<<<<<<vvxxxxv<^^^>>vv>vv>v\n",
      "\t^^^^^<vvv>v>><^^^xx>>>>vvvv\n",
      "\t^x^^^vvv><<xxx^<xxvxx>>v>vv\n",
      "\t^xxx>v>><v^vxxxx>vvvxxx>>vv\n",
      "\t>>><<<><<<<<<<>>>>>>>>>>>>G\n"
     ]
    }
   ],
   "source": [
    "pretty_print_policy(full_world, goal, full_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Value Iteration (if submitting)\n",
    "\n",
    "Provide implementation and output of policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Helper Functions ##\n",
    "\n",
    "** getQs(r, c, Rs, V_last, T_unplanned, gamma) **  \n",
    "This function calculates the Q-value for the value iteration algorithm for a specific state. \n",
    "\n",
    "The function uses a doubly-nested loop of each transitions from the state, and additively calculates the Q by using gamma, the probability of planned move, and the corresponding V-value from the last iteration. \n",
    "\n",
    "** maxDiffInV(v1, v2) **  \n",
    "Given two set of V values, this function calculates the largest difference for all states. This is used to derive the convergence creteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getQs(r, c, Rs, V_last, T_unplanned, gamma):\n",
    "    Q = [0 for tmp in Rs]\n",
    "    T_planned = 1 - (3-len(Rs))*T_unplanned # probability of planned move\n",
    "\n",
    "    for m,(tmp,reward) in enumerate(Rs):\n",
    "        Q[m] += reward\n",
    "        for n,(xy,tmp2) in enumerate(Rs):\n",
    "            pr = T_planned if m==n else T_unplanned\n",
    "            x,y = (r+xy[1], c+xy[0])\n",
    "            Q[m] += gamma * (pr * V_last[x][y])\n",
    "    return Q\n",
    "\n",
    "def maxDiffInV(v1, v2):\n",
    "    tmp = zip([x for v in v1 for x in v], [x for v in v2 for x in v])\n",
    "    return max([abs(x-y) for x,y in tmp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Program ###\n",
    "\n",
    "The function starts off by pre-calculating all valid transitions for all states. It uses a hard-coded convergence criteria of 1E-10 of largest utility differences and the probability of taking an unplanned move of 10%. \n",
    "\n",
    "Based on the Value Iteration algorithm, the program loops through all states, and for all valid transition of each states, it updates the Q-value based on the reward associated for each transition. \n",
    "\n",
    "At the end of one iteration, it calculates the maximum utility differences of all states, and stops the algorithm when the convergence criteria is reached. The convergence criteria is that the maximum difference be lower than 1E-10, or 1000 iterations, whichever is reached first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(world, costs, goal, reward, actions, gamma):\n",
    "    Rs = transCosts(world, actions, costs, goal, reward)\n",
    "    eps = 1E-10\n",
    "    T = 0.1\n",
    "    \n",
    "    V = [[0 for c in r] for r in world] # pre-allocate with zeroes\n",
    "    #V[goal[0]][goal[1]] = reward # define reward\n",
    "    Q = [[0 for c in r] for r in world] # pre-allocate with zeroes\n",
    "    pols = dict() # dict for storing policy moves\n",
    "    \n",
    "    convg, t = (False, 0) # loop stoppage criteria\n",
    "    while (not convg) and (t < 1000): # max 1000 iterations if not converge\n",
    "        V_last = copy.deepcopy(V) # copy V_last\n",
    "        for r,row in enumerate(world):\n",
    "            for c,terr in enumerate(row):\n",
    "                if (world[r][c]=='x') or ((r,c)==goal):\n",
    "                    continue # skip impassible or goal states\n",
    "                Q[r][c] = getQs(r, c, Rs[r][c], V_last, T, gamma)\n",
    "                V[r][c], pols[(r,c)] = getPolicyFromQ(Q[r][c], Rs[r][c])\n",
    "                \n",
    "        maxDiff = maxDiffInV(V, V_last)\n",
    "        convg,t = (maxDiff < eps),t+1\n",
    "        # print('Iter #' + repr(t) + ', max utility diff = ' + repr(maxDiff) )\n",
    "    print('Total iterations: '+repr(t)+', max util. diff = '+ repr(maxDiff) )\n",
    "    return pols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 75, max util. diff = 9.526246458335663e-11\n",
      "\tvxvvvx\n",
      "\tv>v>v<\n",
      "\t>>>>vv\n",
      "\t>>^>>v\n",
      "\t^^^x>G\n"
     ]
    }
   ],
   "source": [
    "reward = 100\n",
    "goal = (4,5)\n",
    "disc = 0.7\n",
    "\n",
    "a = value_iteration( test_world, costs, goal, reward, cardinal_moves, disc)\n",
    "pretty_print_policy( test_world, goal, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 175, max util. diff = 7.567280135845067e-11\n",
      "\tvvvvvvvvvvvvvvvvv<<<<<>>>vv\n",
      "\t>vv<>>>>>vvvvvvv<<xxxxxxxvv\n",
      "\t>vv<xx^>>>>>>>vvvxxxvv<xxvv\n",
      "\t>vv<<xxx>>>>>>>vvvvvv<<xxvv\n",
      "\t>v<<<xxvv>>>>>>>>vvvvvxxxvv\n",
      "\t>v<<xxvvvv^^^^>>>>>vvvvxvv<\n",
      "\t>vvxxv>vvv<^xxx^>>>>vvvvvv<\n",
      "\t>vvvvvvvv<<<<<xxx>>>>>>vvv<\n",
      "\t>>v>>>vv<<<<<<xx>>>>>>>>vv<\n",
      "\t>>>>>>vv<<<xxxx>>>>^^^>>vv<\n",
      "\t>>>>>vvv<<xxxv>>>^^^xxx>vv<\n",
      "\t>>>>>vvv<xxvv>>>^^^<<xx>vv<\n",
      "\t>>>>>>vvvxxvvv^^>>^vvx>>vv<\n",
      "\t>>^^>>>vvvvv<<<>>>>>>>>>vv<\n",
      "\t>>^^x>>>vvv<<^^^>>>^^^x>vv<\n",
      "\t>^^xxx>>vvvvxxx^>^^^^xxvvv<\n",
      "\t^^xx>>>>>vvvvvxxx^^xxxvvv<<\n",
      "\t>^<xx>>>>>>vvvvvxxxxvvvvv<<\n",
      "\t>^<xxx>>>>>>>>>vvvvvvvvvvv<\n",
      "\t>^<<xxx>>>>^^^>>>v>>>>vvvv<\n",
      "\t>^<<<vxx^^^^^^x>>>>v>>vvvv<\n",
      "\t>^<<<vvxxx^^xx>>>>>>>>>vvv<\n",
      "\t>^^<<<vvvxxxx>>>^^>>>>>>vv<\n",
      "\t^^^^^>vv>>>>>>^^^xx^>>>>vv<\n",
      "\t^x^^>>>>>>^xxx^^xxvxx^>>>vv\n",
      "\t^xxx>>^^^^^<xxxx>>vvxxx>>vv\n",
      "\t^>>>>^^^^^^<<<>>>>>>>>>>>>G\n"
     ]
    }
   ],
   "source": [
    "reward = 1000\n",
    "goal = (26,26)\n",
    "disc = 0.7\n",
    "\n",
    "b = value_iteration( full_world, costs, goal, reward, cardinal_moves, disc)\n",
    "pretty_print_policy( full_world, goal, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
