{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import itertools, random, time\n",
    "from Bresenham import getPath\n",
    "from utilities import getTransitions, getVPolicyFromQ, readTrackFile\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelList = [1,0,-1]\n",
    "velocList = range(-5,6)\n",
    "accels = list(itertools.product(accelList,accelList))\n",
    "velocs = list(itertools.product(velocList,velocList))\n",
    "actsPrint = {(0,0): \"o\", (1,1): \"\\N{North East Arrow}\", \n",
    "             (1,0): \"\\N{Rightwards Arrow}\", (1,-1): \"\\N{South East Arrow}\",\n",
    "             (0,-1): \"\\N{Downwards Arrow}\", (-1,-1): \"\\N{South West Arrow}\",\n",
    "             (-1,0): \"\\N{Leftwards Arrow}\", (-1,1): \"\\N{North West Arrow}\",\n",
    "             (0,1): \"\\N{Upwards Arrow}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fPath = os.path.join('data','O-track.txt')\n",
    "O_states,O_goals,O_track,O_starts = readTrackFile(fPath, velocs)\n",
    "O_TRs = getTransitions(O_states, accels, O_track, O_starts) # all trans from all states\n",
    "O_TRs_c = getTransitions(O_states, accels, O_track, O_starts,hardCrash=True) # all trans from all states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcQfromV(state, tr, V_0, failPr, gamma):\n",
    "    Q = {k:0 for k in tr} # initialize Q to all 0 for all actions\n",
    "    failSt = tr[(0,0)][0] # resultant state of a failed acceleration\n",
    "    \n",
    "    for accel,(newState,reward) in tr.items(): # loop over all transitions\n",
    "        tmp = (1-failPr)*V_0[newState] + failPr*V_0[failSt] # sum of pr*V\n",
    "        Q[accel] = reward + gamma * tmp # E[r|s,a] + gamma * sum(V)\n",
    "    return Q\n",
    "\n",
    "def maxDiffTwoVs(v1, v2):\n",
    "    pairedVals = zip(v1.values(), v2.values()) # get all values from both V vals\n",
    "    return max([abs(x-y) for x,y in pairedVals]) # return max diff of all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def valueIteration(states, TRs_all, gamma=0.9, eps=1e-9, pr_fail=0.2, trace=False):\n",
    "    Vs = {k:0 for k in states}\n",
    "    Qs = {k:None for k in states}\n",
    "    pols = dict()\n",
    "    \n",
    "    for t in itertools.count(): # loop until converged\n",
    "        Vs_old = Vs.copy() # copy of Vs as old V values\n",
    "        for st in states: # loop over all states\n",
    "            Qs[st] = calcQfromV(st, TRs_all[st], Vs_old, pr_fail, gamma)\n",
    "            Vs[st],pols[st] = getVPolicyFromQ(Qs[st])\n",
    "        \n",
    "        maxDiff = maxDiffTwoVs(Vs, Vs_old)\n",
    "        if trace and (t % 10 == 0):\n",
    "            print('[%05d]: diff=%f'%(t,maxDiff))\n",
    "        \n",
    "        if (maxDiff<eps) or (t >= 1e4): # max 1000 iters if not converged\n",
    "            break\n",
    "\n",
    "    if trace:\n",
    "        print('Total iters: %d, max util diff = %f'%(t,maxDiff))\n",
    "    return pols, t, maxDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_VI,b,c = valueIteration(O_states, O_TRs, trace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsGreedy(Q, acts, temp=0.5):\n",
    "    qs = np.array( [Q[k] for k in acts] ) # all Q(s,a) values\n",
    "    P_a = np.exp(qs / temp) # numerator of softmax, exp[Q(s,a)]\n",
    "    P_a = P_a / P_a.sum() # array of probabilites\n",
    "    \n",
    "    idx = np.argmax(np.random.random() < P_a.cumsum())\n",
    "    return acts[idx]\n",
    "\n",
    "def qEpisode(st, Q, TRs, gm, et, prF, trk, trace):\n",
    "    for t in itertools.count(): # loop for exploration\n",
    "        if trk[st[1],st[0]] == 'F': # if curr state is a goal\n",
    "            break\n",
    "\n",
    "        tr = TRs[st] # all possible transitions from curr state\n",
    "        temp = max(0.05, 1/np.exp(t/150)**2) # temperature schedule\n",
    "\n",
    "        attempt = epsGreedy(Q[st], list(tr.keys()), temp) # eps-greedy attemp\n",
    "        if random.random() < prF: # failed to accelerate\n",
    "            actual = (0,0) # fail to accelerate\n",
    "        else: # successfully change accelerattion\n",
    "            actual = attempt \n",
    "            \n",
    "        newSt,reward = tr[actual] # next state and reward for action\n",
    "        maxQ = max( Q[newSt][k] for k in TRs[newSt].keys() ) # Q of new state\n",
    "        Q[st][actual] += et*(reward + gm*maxQ - Q[st][actual]) # update Q(s,a)\n",
    "        st = newSt\n",
    "\n",
    "    return Q, t # return Q values and number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def qLearning(states, accels, track, TRs, nEpisodes=100000, gamma=0.9, eta=0.1,\n",
    "              pr_fail=0.2, trace=None):\n",
    "    states = list(states)\n",
    "    #states = [x for x in states if track[x[1],x[0]]!='F'] # remove goal states\n",
    "    \n",
    "    Qs = dict()\n",
    "    for st in states: # initialize Q table to all 0's\n",
    "        Qs[st] = {a: 0 for a in accels}\n",
    "        \n",
    "    epsLen = np.zeros(nEpisodes, int) # length of each episode\n",
    "    for ep in range(nEpisodes):\n",
    "        start = random.choice(states) # choose random starting location\n",
    "        # run one episode of q-learning, get new Qs, ep. len, ep. cum. reward\n",
    "        Qs,epsLen[ep] = qEpisode(start, Qs, TRs, gamma, eta, pr_fail,\n",
    "                                 track, trace)\n",
    "        if trace and (ep%trace)==0:\n",
    "            print('[%05d] Episode len = %d'%(ep,epsLen[ep]))\n",
    "    if trace:\n",
    "        print('[%05d] Last episode, len = %d'%(ep,epsLen[ep]))\n",
    "    \n",
    "    policy = dict() # pre-allocate policy\n",
    "    for st in states: # loop over all states to get policy for each state\n",
    "        tmp, policy[st] = getVPolicyFromQ(Qs[st]) # best policy accord. to Qs\n",
    "    return policy, epsLen, Qs # return policy and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00000] Episode len = 1308\n",
      "[01000] Episode len = 381\n",
      "[02000] Episode len = 218\n",
      "[03000] Episode len = 457\n",
      "[04000] Episode len = 0\n",
      "[05000] Episode len = 319\n",
      "[06000] Episode len = 327\n",
      "[07000] Episode len = 421\n",
      "[08000] Episode len = 336\n",
      "[09000] Episode len = 16\n",
      "[09999] Last episode, len = 393\n"
     ]
    }
   ],
   "source": [
    "pol,ll,qq = qLearning(O_states, accels, O_track, O_TRs, nEpisodes=10000, trace=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5196386564674706"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = np.exp(np.array([qq[(20, 17, 0, 0)][k] for k in accels]) /0.05)\n",
    "(aa/aa.sum()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TRss[(21,17,0,-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRss = getTransitions(O_states,accels,O_track)\n",
    "\n",
    "z = transFromState((2,15,1,-2),accels,O_track)\n",
    "list(z.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(O_starts,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackToStr(track, state):\n",
    "    trk = track.copy()\n",
    "    trk[state[1],state[0]] = 'X'\n",
    "    s = '\\n'.join([''.join(l) for l in trk]) + '\\n'\n",
    "    s += 'x-vel=%d, y-vel=%d\\n'%state[2:]\n",
    "    return s\n",
    "\n",
    "################################################################################\n",
    "def simulateRace(trk, TRs, starts, pol, p_fail=0.2, trace=False):\n",
    "    stepLimit = np.prod(trk.shape)*2/(1-p_fail) # step limit for non-viability\n",
    "    \n",
    "    if len(starts) > 1:\n",
    "        startPt = random.choice(starts) # choose starting point randomly\n",
    "    if len(startPt) != 4: # only location provided\n",
    "        st = startPt + (0,0) # set starting state, 0 velocity\n",
    "    viable = False\n",
    "    for t in itertools.count(): # counter for total distance\n",
    "        if trace:\n",
    "            clear_output(wait=True)\n",
    "            print(trackToStr(trk,st))\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        if trk[st[1],st[0]] == 'F': # if curr state is a goal\n",
    "            viable = True\n",
    "            break\n",
    "        if t > stepLimit: # too many steps, not viable policy\n",
    "            break\n",
    "        \n",
    "        if random.random() < p_fail: # failed to accelerate\n",
    "            accel = (0,0)\n",
    "        else:\n",
    "            accel = pol[st] # optimal accel according to policy\n",
    "            \n",
    "        st = TRs[st][accel][0] # set new state to result of transition\n",
    "    return t, startPt, viable # return steps taken, start pt, and viability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(len(pol), int)\n",
    "b = np.zeros(len(pol), bool)\n",
    "\n",
    "for n,st in enumerate(pol.keys()):\n",
    "    a[n],tmp,b[n] = simulateRace(O_track, O_TRs, st, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulateRace(O_track, O_TRs, O_starts, pol, p_fail=0.2, trace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 5.4)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0.2, 5.4])\n",
    "tuple(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
